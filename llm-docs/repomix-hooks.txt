This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: hooks
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------

================================================================
Directory Structure
================================================================
hooks/use-chat.ts
hooks/use-executor.ts
hooks/use-media-query.ts
hooks/use-memory-provider.ts
hooks/use-mobile.tsx
hooks/use-supabase-crud.ts
hooks/use-supabase-direct.ts
hooks/use-supabase-fetch.ts
hooks/use-supabase-realtime.ts
hooks/use-toast.ts
hooks/use-upstash-adapter.ts

================================================================
Files
================================================================

================
File: hooks/use-media-query.ts
================
"use client"

import { useEffect, useState } from "react"

/**
 * Custom hook for responsive design that detects if a media query matches
 * 
 * @param query - CSS media query string
 * @returns boolean indicating if the media query matches
 * 
 * @example
 * ```tsx
 * const isMobile = useMediaQuery("(max-width: 768px)")
 * ```
 */
export function useMediaQuery(query: string): boolean {
  const [matches, setMatches] = useState<boolean>(false)
  
  useEffect(() => {
    // Check if window is defined (client-side)
    if (typeof window !== "undefined") {
      const media = window.matchMedia(query)
      
      // Set initial value
      setMatches(media.matches)
      
      // Define listener function
      const listener = (event: MediaQueryListEvent) => {
        setMatches(event.matches)
      }
      
      // Add listener
      media.addEventListener("change", listener)
      
      // Clean up
      return () => {
        media.removeEventListener("change", listener)
      }
    }
    
    // Default to false on server-side
    return () => {}
  }, [query])
  
  return matches
}

================
File: hooks/use-memory-provider.ts
================
'use client';

/**
 * Hook for determining the current memory provider
 *
 * This hook provides information about the current memory provider configuration
 * based on environment variables. It helps components and other hooks determine
 * whether to use Supabase or Upstash for data storage.
 *
 * @module hooks/use-memory-provider
 */

import { useState, useEffect } from 'react';

/**
 * Memory provider types
 */
export type MemoryProvider = 'libsql' | 'upstash' | 'supabase';

/**
 * Memory provider configuration
 */
export interface MemoryProviderConfig {
  /**
   * The current memory provider
   */
  provider: MemoryProvider;

  /**
   * Whether Upstash adapter is enabled for Supabase compatibility
   */
  useUpstashAdapter: boolean;

  /**
   * Whether Upstash Redis is available
   */
  isRedisAvailable: boolean;

  /**
   * Whether Upstash Vector is available
   */
  isVectorAvailable: boolean;

  /**
   * Whether the memory provider is ready to use
   */
  isReady: boolean;

  /**
   * Error message if any
   */
  error?: string;
}

/**
 * Hook for determining the current memory provider
 * @returns Memory provider configuration
 */
export function useMemoryProvider(): MemoryProviderConfig {
  const [config, setConfig] = useState<MemoryProviderConfig>({
    provider: 'libsql',
    useUpstashAdapter: false,
    isRedisAvailable: false,
    isVectorAvailable: false,
    isReady: false
  });

  useEffect(() => {
    // Fetch memory provider configuration from the server
    async function fetchConfig() {
      try {
        const response = await fetch('/api/memory/config', {
          // Add cache control headers to prevent caching
          headers: {
            'Cache-Control': 'no-cache, no-store, must-revalidate',
            'Pragma': 'no-cache',
            'Expires': '0'
          }
        });

        if (!response.ok) {
          throw new Error(`Failed to fetch memory provider configuration: ${response.statusText}`);
        }

        const data = await response.json();
        setConfig({
          provider: data.provider || 'libsql',
          useUpstashAdapter: data.useUpstashAdapter || false,
          isRedisAvailable: data.isRedisAvailable || false,
          isVectorAvailable: data.isVectorAvailable || false,
          isReady: data.isReady || false,
          error: data.error
        });

        // Log configuration for debugging
        console.log('Memory provider configuration:', {
          provider: data.provider,
          useUpstashAdapter: data.useUpstashAdapter,
          isRedisAvailable: data.isRedisAvailable,
          isVectorAvailable: data.isVectorAvailable
        });
      } catch (error) {
        console.error('Error fetching memory provider configuration:', error);
        setConfig(prev => ({
          ...prev,
          isReady: true,
          error: error instanceof Error ? error.message : 'Unknown error'
        }));
      }
    }

    fetchConfig();

    // Refresh configuration every 5 minutes to detect changes
    const intervalId = setInterval(fetchConfig, 5 * 60 * 1000);

    // Clean up interval on unmount
    return () => clearInterval(intervalId);
  }, []);

  return config;
}

/**
 * Utility function to determine if Upstash should be used
 * @returns Whether Upstash should be used
 */
export function shouldUseUpstash(): boolean {
  // Check if we're in a browser environment
  if (typeof window !== 'undefined') {
    // In browser, we need to rely on the API to tell us
    return false; // Default to false, will be updated by the hook
  }

  // In server environment, we can check environment variables directly
  return process.env.USE_UPSTASH_ADAPTER === 'true' &&
         process.env.UPSTASH_REDIS_REST_URL !== undefined &&
         process.env.UPSTASH_REDIS_REST_TOKEN !== undefined;
}

/**
 * Utility function to determine if Upstash Vector is available
 * @returns Whether Upstash Vector is available
 */
export function isUpstashVectorAvailable(): boolean {
  // Check if we're in a browser environment
  if (typeof window !== 'undefined') {
    // In browser, we need to rely on the API to tell us
    return false; // Default to false, will be updated by the hook
  }

  // In server environment, we can check environment variables directly
  return process.env.UPSTASH_VECTOR_REST_URL !== undefined &&
         process.env.UPSTASH_VECTOR_REST_TOKEN !== undefined;
}

export default useMemoryProvider;

================
File: hooks/use-mobile.tsx
================
import * as React from "react"

const MOBILE_BREAKPOINT = 768

export function useIsMobile() {
  const [isMobile, setIsMobile] = React.useState<boolean | undefined>(undefined)

  React.useEffect(() => {
    const mql = window.matchMedia(`(max-width: ${MOBILE_BREAKPOINT - 1}px)`)
    const onChange = () => {
      setIsMobile(window.innerWidth < MOBILE_BREAKPOINT)
    }
    mql.addEventListener("change", onChange)
    setIsMobile(window.innerWidth < MOBILE_BREAKPOINT)
    return () => mql.removeEventListener("change", onChange)
  }, [])

  return !!isMobile
}

================
File: hooks/use-upstash-adapter.ts
================
'use client';

/**
 * Hook for accessing Upstash adapter configuration
 *
 * This hook provides access to Upstash adapter configuration and utilities
 * for interacting with Upstash Redis and Vector services as a drop-in
 * replacement for Supabase.
 *
 * @module hooks/use-upstash-adapter
 */

import { useState, useEffect } from 'react';
import { useMemoryProvider } from './use-memory-provider';

/**
 * Upstash adapter configuration
 */
export interface UpstashAdapterConfig {
  /**
   * Whether Upstash adapter is enabled
   */
  enabled: boolean;

  /**
   * Whether Upstash Redis is available
   */
  redisAvailable: boolean;

  /**
   * Whether Upstash Vector is available
   */
  vectorAvailable: boolean;

  /**
   * Whether the adapter is ready to use
   */
  isReady: boolean;

  /**
   * Error message if any
   */
  error?: string;

  /**
   * Upstash Redis URL
   */
  redisUrl?: string;

  /**
   * Upstash Vector URL
   */
  vectorUrl?: string;
}

/**
 * Hook for accessing Upstash adapter configuration
 * @returns Upstash adapter configuration
 */
export function useUpstashAdapter(): UpstashAdapterConfig {
  const memoryProvider = useMemoryProvider();
  const [config, setConfig] = useState<UpstashAdapterConfig>({
    enabled: memoryProvider.useUpstashAdapter,
    redisAvailable: memoryProvider.isRedisAvailable,
    vectorAvailable: memoryProvider.isVectorAvailable,
    isReady: memoryProvider.isReady
  });

  useEffect(() => {
    // Update config when memory provider changes
    setConfig({
      enabled: memoryProvider.useUpstashAdapter,
      redisAvailable: memoryProvider.isRedisAvailable,
      vectorAvailable: memoryProvider.isVectorAvailable,
      isReady: memoryProvider.isReady,
      error: memoryProvider.error
    });

    // Fetch additional Upstash adapter configuration if enabled
    if (memoryProvider.useUpstashAdapter && memoryProvider.isReady) {
      async function fetchAdapterConfig() {
        try {
          const response = await fetch('/api/memory/upstash-config', {
            // Add cache control headers to prevent caching
            headers: {
              'Cache-Control': 'no-cache, no-store, must-revalidate',
              'Pragma': 'no-cache',
              'Expires': '0'
            }
          });

          if (!response.ok) {
            throw new Error(`Failed to fetch Upstash adapter configuration: ${response.statusText}`);
          }

          const data = await response.json();

          if (!data.enabled) {
            throw new Error('Upstash adapter is not enabled on the server');
          }

          setConfig(prev => ({
            ...prev,
            redisUrl: data.redisUrl,
            vectorUrl: data.vectorUrl,
            qstashEnabled: data.qstashEnabled,
            isReady: true
          }));

          // Log configuration for debugging
          console.log('Upstash adapter configuration:', {
            redisUrl: data.redisUrl ? '✓ Available' : '✗ Not available',
            vectorUrl: data.vectorUrl ? '✓ Available' : '✗ Not available',
            qstashEnabled: data.qstashEnabled ? '✓ Enabled' : '✗ Disabled'
          });
        } catch (error) {
          console.error('Error fetching Upstash adapter configuration:', error);
          setConfig(prev => ({
            ...prev,
            error: error instanceof Error ? error.message : 'Unknown error'
          }));
        }
      }

      fetchAdapterConfig();
    }
  }, [memoryProvider]);

  return config;
}

/**
 * Utility function to determine if Upstash adapter should be used
 * @returns Whether Upstash adapter should be used
 */
export function shouldUseUpstashAdapter(): boolean {
  // Use the memory provider utility
  return useMemoryProvider().useUpstashAdapter;
}

/**
 * Utility function to check if Upstash Redis is available
 * @returns Whether Upstash Redis is available
 */
export function isUpstashRedisAvailable(): boolean {
  return useMemoryProvider().isRedisAvailable;
}

/**
 * Utility function to check if Upstash Vector is available
 * @returns Whether Upstash Vector is available
 */
export function isUpstashVectorAvailable(): boolean {
  return useMemoryProvider().isVectorAvailable;
}

export default useUpstashAdapter;

================
File: hooks/use-executor.ts
================
"use client"

import { useState, useRef } from "react"
import { useToast } from "@/hooks/use-toast"
import { LRUCache } from 'lru-cache'

interface UseAgentExecutorOptions {
  agentId: string
  onSuccess?: (data: any) => void
  onError?: (error: Error) => void
}

interface UseToolExecutorOptions {
  toolId: string
  onSuccess?: (data: any) => void
  onError?: (error: Error) => void
}

export function useAgentExecutor<T = any>({ agentId, onSuccess, onError }: UseAgentExecutorOptions) {
  const { toast } = useToast()
  const [isExecuting, setIsExecuting] = useState(false)
  const [error, setError] = useState<Error | null>(null)
  const cache = useRef(new LRUCache<string, any>({
    max: 50,
    ttl: 300000, // 5 minutes
  }))
  const abortControllerRef = useRef<AbortController | null>(null)

  const executeAgent = async (message: string, history: any[] = [], retryCount = 0) => {
    setIsExecuting(true);
    setError(null);
    
    try {
      // Cancel previous request if still in progress
      if (abortControllerRef.current) {
        abortControllerRef.current.abort()
      }
      
      // Create new abort controller
      abortControllerRef.current = new AbortController()
      
      // Generate cache key based on agent ID, message, and history
      const cacheKey = `${agentId}_${message}_${JSON.stringify(history)}`
      
      // Check cache
      const cachedResult = cache.current.get(cacheKey)
      if (cachedResult) return cachedResult
      
      const response = await fetch(`/api/agents/${agentId}/run`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          message,
          history,
        }),
        signal: abortControllerRef.current.signal
      })

      if (!response.ok) {
        const errorData = await response.json()
        throw new Error(errorData.error || "Failed to execute agent")
      }

      const data = await response.json()

      if (onSuccess) {
        onSuccess(data)
      }

      cache.current.set(cacheKey, data)
      return data
    } catch (err) {
      const error = err instanceof Error ? err : new Error("Failed to execute agent")
      
      // Check if error is retryable (network error, 5xx)
      const isRetryable = error.message.includes('network') || 
                          error.message.includes('5') || 
                          error.message.includes('timeout')
      
      if (isRetryable && retryCount < 3) {
        // Exponential backoff
        const delay = 1000 * Math.pow(2, retryCount)
        await new Promise(resolve => setTimeout(resolve, delay))
        return executeAgent(message, history, retryCount + 1)
      }
      
      // Handle non-retryable error
      setError(error)

      toast({
        title: "Error executing agent",
        description: error.message,
        variant: "destructive",
      })

      if (onError) {
        onError(error)
      }

      throw error
    } finally {
      setIsExecuting(false)
    }
  }

  const executeAgentWithStream = async (
    message: string, 
    history: any[] = [], 
    onChunk: (chunk: string) => void
  ) => {
    setIsExecuting(true);
    setError(null);
    
    try {
      const response = await fetch(`/api/agents/${agentId}/run`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ message, history, stream: true }),
      });
      
      if (!response.ok) {
        // Handle error
        // ...
      }
      
      const reader = response.body?.getReader();
      if (!reader) throw new Error("Response body is null");
      
      const decoder = new TextDecoder();
      let result = '';
      
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        
        const chunk = decoder.decode(value, { stream: true });
        result += chunk;
        onChunk(chunk);
      }
      
      if (onSuccess) {
        onSuccess(JSON.parse(result));
      }
      
      return JSON.parse(result);
    } catch (err) {
      // Handle error
      // ...
    } finally {
      setIsExecuting(false);
    }
  }

  return {
    executeAgent,
    executeAgentWithStream,
    isExecuting,
    error,
    cancel: () => abortControllerRef.current?.abort()
  }
}

export function useToolExecutor({ toolId, onSuccess, onError }: UseToolExecutorOptions) {
  const { toast } = useToast()
  const [isExecuting, setIsExecuting] = useState(false)
  const [error, setError] = useState<Error | null>(null)

  const executeTool = async (parameters: Record<string, any>) => {
    setIsExecuting(true)
    setError(null)

    try {
      const response = await fetch("/api/tools/execute", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          toolId,
          parameters,
        }),
      })

      if (!response.ok) {
        const errorData = await response.json()
        throw new Error(errorData.error || "Failed to execute tool")
      }

      const data = await response.json()

      if (onSuccess) {
        onSuccess(data)
      }

      toast({
        title: "Tool executed successfully",
        description: "The tool completed execution",
      })

      return data
    } catch (err) {
      const error = err instanceof Error ? err : new Error("Failed to execute tool")
      setError(error)

      toast({
        title: "Error executing tool",
        description: error.message,
        variant: "destructive",
      })

      if (onError) {
        onError(error)
      }

      throw error
    } finally {
      setIsExecuting(false)
    }
  }

  return {
    executeTool,
    isExecuting,
    error,
  }
}

================
File: hooks/use-toast.ts
================
"use client"

/**
 * Enhanced toast hook with advanced features
 * Supports multiple toast types, actions, and toast queue management
 *
 * @module hooks/use-toast
 */

// Inspired by react-hot-toast library
import * as React from "react"

import type {
  ToastActionElement,
  ToastProps,
} from "@/components/ui/toast"

// Configuration
const TOAST_LIMIT = 5 // Increased from 1 to allow multiple toasts
const TOAST_REMOVE_DELAY = 5000 // Reduced from 1000000 to 5000ms (5 seconds)

/**
 * Toast variant types
 */
export type ToastVariant =
  | "default"
  | "destructive"

/**
 * Toast priority levels
 */
export type ToastPriority =
  | "low"
  | "normal"
  | "high"
  | "urgent"

/**
 * Toast action with callback
 */
export interface ToastAction {
  label: string
  onClick: () => void
  className?: string
}

/**
 * Enhanced toast properties
 */
type ToasterToast = ToastProps & {
  id: string
  title?: React.ReactNode
  description?: React.ReactNode
  action?: ToastActionElement
  variant?: ToastVariant
  duration?: number
  priority?: ToastPriority
  icon?: React.ReactNode
  onDismiss?: () => void
  group?: string
  createdAt: number
}

/**
 * Toast action types
 */
const actionTypes = {
  ADD_TOAST: "ADD_TOAST",
  UPDATE_TOAST: "UPDATE_TOAST",
  DISMISS_TOAST: "DISMISS_TOAST",
  REMOVE_TOAST: "REMOVE_TOAST",
  DISMISS_ALL_TOASTS: "DISMISS_ALL_TOASTS",
  REMOVE_ALL_TOASTS: "REMOVE_ALL_TOASTS",
  DISMISS_GROUP: "DISMISS_GROUP",
  PAUSE_TOAST: "PAUSE_TOAST",
  RESUME_TOAST: "RESUME_TOAST",
} as const

let count = 0

function genId() {
  count = (count + 1) % Number.MAX_SAFE_INTEGER
  return count.toString()
}

type ActionType = typeof actionTypes

/**
 * Toast actions
 */
type Action =
  | {
      type: ActionType["ADD_TOAST"]
      toast: ToasterToast
    }
  | {
      type: ActionType["UPDATE_TOAST"]
      toast: Partial<ToasterToast>
    }
  | {
      type: ActionType["DISMISS_TOAST"]
      toastId?: ToasterToast["id"]
    }
  | {
      type: ActionType["REMOVE_TOAST"]
      toastId?: ToasterToast["id"]
    }
  | {
      type: ActionType["DISMISS_ALL_TOASTS"]
    }
  | {
      type: ActionType["REMOVE_ALL_TOASTS"]
    }
  | {
      type: ActionType["DISMISS_GROUP"]
      group: string
    }
  | {
      type: ActionType["PAUSE_TOAST"]
      toastId: ToasterToast["id"]
    }
  | {
      type: ActionType["RESUME_TOAST"]
      toastId: ToasterToast["id"]
    }

/**
 * Toast state
 */
interface State {
  toasts: ToasterToast[]
  paused: Record<string, boolean>
  queue: ToasterToast[]
}

/**
 * Toast timeout management
 */
const toastTimeouts = new Map<string, ReturnType<typeof setTimeout>>()

/**
 * Add toast to removal queue
 */
const addToRemoveQueue = (toastId: string, duration?: number) => {
  // Clear existing timeout if any
  if (toastTimeouts.has(toastId)) {
    clearTimeout(toastTimeouts.get(toastId))
    toastTimeouts.delete(toastId)
  }

  // Don't set a timeout for permanent toasts (duration === 0)
  if (duration === 0) return

  // Set timeout to remove toast
  const timeout = setTimeout(() => {
    toastTimeouts.delete(toastId)
    dispatch({
      type: "REMOVE_TOAST",
      toastId: toastId,
    })

    // Process queue after removing a toast
    processQueue()
  }, duration || TOAST_REMOVE_DELAY)

  toastTimeouts.set(toastId, timeout)
}

/**
 * Process toast queue
 */
const processQueue = () => {
  const state = memoryState

  // If we have space for more toasts and there are toasts in the queue
  if (state.toasts.length < TOAST_LIMIT && state.queue.length > 0) {
    // Sort queue by priority and creation time
    const sortedQueue = [...state.queue].sort((a, b) => {
      const priorityOrder = { urgent: 3, high: 2, normal: 1, low: 0 }
      const aPriority = priorityOrder[a.priority || 'normal']
      const bPriority = priorityOrder[b.priority || 'normal']

      if (aPriority !== bPriority) {
        return bPriority - aPriority // Higher priority first
      }

      return a.createdAt - b.createdAt // Older first
    })

    // Get the next toast from the queue
    const nextToast = sortedQueue[0]

    // Remove it from the queue and add it to active toasts
    dispatch({
      type: "ADD_TOAST",
      toast: nextToast,
    })
  }
}

/**
 * Toast reducer
 */
export const reducer = (state: State, action: Action): State => {
  switch (action.type) {
    case "ADD_TOAST": {
      // If we've reached the toast limit, add to queue instead
      if (state.toasts.length >= TOAST_LIMIT) {
        return {
          ...state,
          queue: [...state.queue, action.toast],
        }
      }

      // Add toast and set timeout for removal
      const toast = action.toast
      if (!state.paused[toast.id]) {
        addToRemoveQueue(toast.id, toast.duration)
      }

      return {
        ...state,
        toasts: [action.toast, ...state.toasts].slice(0, TOAST_LIMIT),
      }
    }

    case "UPDATE_TOAST":
      return {
        ...state,
        toasts: state.toasts.map((t) =>
          t.id === action.toast.id ? { ...t, ...action.toast } : t
        ),
      }

    case "DISMISS_TOAST": {
      const { toastId } = action

      // Call onDismiss callback if defined
      if (toastId) {
        const toast = state.toasts.find(t => t.id === toastId)
        if (toast?.onDismiss) {
          toast.onDismiss()
        }
        addToRemoveQueue(toastId)
      } else {
        state.toasts.forEach((toast) => {
          if (toast.onDismiss) {
            toast.onDismiss()
          }
          addToRemoveQueue(toast.id)
        })
      }

      return {
        ...state,
        toasts: state.toasts.map((t) =>
          t.id === toastId || toastId === undefined
            ? {
                ...t,
                open: false,
              }
            : t
        ),
      }
    }

    case "REMOVE_TOAST": {
      if (action.toastId === undefined) {
        return {
          ...state,
          toasts: [],
        }
      }

      // Remove the toast
      const newState = {
        ...state,
        toasts: state.toasts.filter((t) => t.id !== action.toastId),
        paused: { ...state.paused },
      }

      // Remove from paused state
      if (action.toastId in newState.paused) {
        delete newState.paused[action.toastId]
      }

      return newState
    }

    case "DISMISS_ALL_TOASTS": {
      // Call onDismiss for all toasts
      state.toasts.forEach(toast => {
        if (toast.onDismiss) {
          toast.onDismiss()
        }
        addToRemoveQueue(toast.id)
      })

      return {
        ...state,
        toasts: state.toasts.map(t => ({ ...t, open: false })),
      }
    }

    case "REMOVE_ALL_TOASTS": {
      // Clear all timeouts
      toastTimeouts.forEach(timeout => clearTimeout(timeout))
      toastTimeouts.clear()

      return {
        ...state,
        toasts: [],
        queue: [],
        paused: {},
      }
    }

    case "DISMISS_GROUP": {
      const { group } = action

      // Call onDismiss for group toasts
      state.toasts
        .filter(t => t.group === group)
        .forEach(toast => {
          if (toast.onDismiss) {
            toast.onDismiss()
          }
          addToRemoveQueue(toast.id)
        })

      return {
        ...state,
        toasts: state.toasts.map(t =>
          t.group === group ? { ...t, open: false } : t
        ),
      }
    }

    case "PAUSE_TOAST": {
      const { toastId } = action

      // Clear the timeout
      if (toastTimeouts.has(toastId)) {
        clearTimeout(toastTimeouts.get(toastId))
        toastTimeouts.delete(toastId)
      }

      return {
        ...state,
        paused: {
          ...state.paused,
          [toastId]: true,
        },
      }
    }

    case "RESUME_TOAST": {
      const { toastId } = action
      const toast = state.toasts.find(t => t.id === toastId)

      if (toast) {
        addToRemoveQueue(toastId, toast.duration)
      }

      const newPaused = { ...state.paused }
      delete newPaused[toastId]

      return {
        ...state,
        paused: newPaused,
      }
    }

    default:
      return state
  }
}

const listeners: Array<(state: State) => void> = []

let memoryState: State = { toasts: [], paused: {}, queue: [] }

function dispatch(action: Action) {
  memoryState = reducer(memoryState, action)
  listeners.forEach((listener) => {
    listener(memoryState)
  })
}

/**
 * Toast options
 */
type ToastOptions = Omit<ToasterToast, "id" | "createdAt">

/**
 * Create a toast
 */
function toast({ ...props }: ToastOptions) {
  const id = genId()
  const createdAt = Date.now()

  // Update toast
  const update = (props: Partial<ToasterToast>) =>
    dispatch({
      type: "UPDATE_TOAST",
      toast: { ...props, id },
    })

  // Dismiss toast
  const dismiss = () => dispatch({ type: "DISMISS_TOAST", toastId: id })

  // Pause toast (stop timer)
  const pause = () => dispatch({ type: "PAUSE_TOAST", toastId: id })

  // Resume toast (restart timer)
  const resume = () => dispatch({ type: "RESUME_TOAST", toastId: id })

  // Create and dispatch toast
  dispatch({
    type: "ADD_TOAST",
    toast: {
      ...props,
      id,
      createdAt,
      open: true,
      onOpenChange: (open) => {
        if (!open) dismiss()
      },
    },
  })

  return {
    id,
    dismiss,
    update,
    pause,
    resume,
  }
}

/**
 * Create an error toast
 */
toast.error = (props: Omit<ToastOptions, "variant">) => {
  return toast({
    ...props,
    variant: "destructive",
  })
}

/**
 * Create a success toast (uses default variant with custom styling)
 */
toast.success = (props: Omit<ToastOptions, "variant">) => {
  return toast({
    ...props,
    variant: "default",
  })
}

/**
 * Create a warning toast (uses default variant with custom styling)
 */
toast.warning = (props: Omit<ToastOptions, "variant">) => {
  return toast({
    ...props,
    variant: "default",
  })
}

/**
 * Create an info toast (uses default variant with custom styling)
 */
toast.info = (props: Omit<ToastOptions, "variant">) => {
  return toast({
    ...props,
    variant: "default",
  })
}

/**
 * Create a permanent toast (doesn't auto-dismiss)
 */
toast.permanent = (props: ToastOptions) => {
  return toast({
    ...props,
    duration: 0,
  })
}

/**
 * Create a toast with an action
 */
toast.action = (props: ToastOptions & { action: ToastAction }) => {
  const { action, ...rest } = props

  // Create a proper React element for the toast action
  const actionElement: ToastActionElement = React.createElement("button", {
    type: "button",
    className: action.className,
    onClick: () => {
      action.onClick();
      dispatch({ type: "DISMISS_TOAST", toastId: undefined });
    },
    children: action.label
  });

  return toast({
    ...rest,
    action: actionElement,
  });
}

/**
 * Enhanced toast hook
 */
function useToast() {
  const [state, setState] = React.useState<State>(memoryState)

  React.useEffect(() => {
    listeners.push(setState)
    return () => {
      const index = listeners.indexOf(setState)
      if (index > -1) {
        listeners.splice(index, 1)
      }
    }
  }, [state])

  return {
    ...state,
    toast,
    dismiss: (toastId?: string) => dispatch({ type: "DISMISS_TOAST", toastId }),
    dismissAll: () => dispatch({ type: "DISMISS_ALL_TOASTS" }),
    removeAll: () => dispatch({ type: "REMOVE_ALL_TOASTS" }),
    dismissGroup: (group: string) => dispatch({ type: "DISMISS_GROUP", group }),
    pause: (toastId: string) => dispatch({ type: "PAUSE_TOAST", toastId }),
    resume: (toastId: string) => dispatch({ type: "RESUME_TOAST", toastId }),
    update: (toastId: string, props: Partial<ToasterToast>) =>
      dispatch({ type: "UPDATE_TOAST", toast: { ...props, id: toastId } }),
  }
}

export { useToast, toast }

================
File: hooks/use-chat.ts
================
"use client"
import { useState, useEffect, useRef, useCallback } from 'react'
import { nanoid } from 'nanoid'
import { toast } from "sonner"
import { LRUCache } from 'lru-cache'
import { LanguageModelV1Middleware } from "ai";
import { RequestMiddleware, ResponseMiddleware } from "@/lib/middleware";

export interface Message {
  id: string
  role: "user" | "assistant" | "system"
  content: string
  timestamp?: string
  isLoading?: boolean
  attachments?: Array<{
    type: string
    url: string
    name: string
  }>
}

interface UseChatOptions {
  initialMessages?: Message[]
  initialThreadId?: string
  onError?: (error: Error) => void
  onResponse?: (response: any) => void
  onFinish?: (messages: Message[]) => void
  apiEndpoint?: string
  cacheOptions?: {
    enabled?: boolean
    ttl?: number
    maxSize?: number
  }
  streamables?: {
    [key: string]: {
      initialValue?: React.ReactNode
      onUpdate?: (value: React.ReactNode) => void
    }
  }
  multistepOptions?: {
    enableToolComposition?: boolean
    contextWindow?: number
    maxSteps?: number
  }
  middleware?: {
    languageModel?: LanguageModelV1Middleware | LanguageModelV1Middleware[]
    request?: any[] // RequestMiddleware | RequestMiddleware[]
    response?: any[] // ResponseMiddleware | ResponseMiddleware[]
  }
  extractReasoning?: boolean
  simulateStreaming?: boolean
  defaultSettings?: {
    temperature?: number
    maxTokens?: number
    providerMetadata?: Record<string, any>
  }
}

export function useChat({
  initialMessages = [],
  initialThreadId,
  onError,
  onResponse,
  onFinish,
  apiEndpoint = "/api/chat",
  cacheOptions = {
    enabled: true,
    ttl: 60_000, // 1 minute default
    maxSize: 100,
  },
  streamables,
  middleware,
}: UseChatOptions = {}) {
  const [messages, setMessages] = useState<Message[]>(initialMessages)
  const [input, setInput] = useState("")
  const [isLoading, setIsLoading] = useState(false)
  const [threadId, setThreadId] = useState<string>(initialThreadId || nanoid())
  const [attachments, setAttachments] = useState<any[]>([])
  const [streamableValues, setStreamableValues] = useState<Record<string, React.ReactNode>>({})
  const abortControllerRef = useRef<AbortController | null>(null)

  const cache = useRef(
    new LRUCache<string, any>({
      max: cacheOptions.maxSize ?? 100,
      ttl: cacheOptions.ttl ?? 60_000,
      updateAgeOnGet: true,
      ttlAutopurge: true,
    })
  )

  // Initialize streamable values
  useEffect(() => {
    if (streamables) {
      const initialValues: Record<string, React.ReactNode> = {}
      Object.entries(streamables).forEach(([key, config]) => {
        initialValues[key] = config.initialValue || null
      })
      setStreamableValues(initialValues)
    }
  }, [streamables])

  // Reset messages when threadId changes
  useEffect(() => {
    if (initialThreadId !== threadId) {
      fetchMessages(threadId)
    }
  }, [threadId])

  // Fetch messages for a thread
  const fetchMessages = async (threadId: string) => {
    try {
      const response = await fetch(`/api/memory_threads/${threadId}/messages`)
      if (!response.ok) throw new Error("Failed to fetch messages")

      const data = await response.json()

      if (data.messages) {
        setMessages(
          data.messages.map((msg: any) => ({
            id: msg.id || nanoid(),
            role: msg.role,
            content: msg.content,
            timestamp: msg.created_at,
          }))
        )
      }
    } catch (error) {
      console.error("Error fetching messages:", error)
      toast.error("Failed to load messages")
      if (onError && error instanceof Error) {
        onError(error)
      }
    }
  }

  // Send a message
  const sendMessage = useCallback(
    async (options: {
      message?: string
      attachments?: any[]
      modelId?: string
      tools?: string[]
      temperature?: number
      maxTokens?: number
      agentId?: string
      middleware?: {
        languageModel?: LanguageModelV1Middleware | LanguageModelV1Middleware[]
        request?: RequestMiddleware | RequestMiddleware[]
        response?: ResponseMiddleware | ResponseMiddleware[]
      }
    } = {}) => {
      const {
        message = input,
        attachments: messageAttachments = attachments,
        modelId,
        tools = [],
        temperature = 0.7,
        maxTokens = 1000,
        agentId,
      } = options

      if (!message.trim() && messageAttachments.length === 0) return

      // Cancel any ongoing request
      if (abortControllerRef.current) {
        abortControllerRef.current.abort()
      }

      // Create new abort controller
      abortControllerRef.current = new AbortController()

      // Add user message to the UI immediately
      const userMessage: Message = {
        id: nanoid(),
        role: "user",
        content: message,
        timestamp: new Date().toISOString(),
        attachments: messageAttachments.length > 0 ? messageAttachments : undefined,
      }

      setMessages((prev) => [...prev, userMessage])

      // Add loading message
      const loadingMessageId = nanoid()
      setMessages((prev) => [
        ...prev,
        {
          id: loadingMessageId,
          role: "assistant",
          content: "",
          isLoading: true,
        },
      ])

      setInput("")
      setAttachments([])
      setIsLoading(true)

      try {
        // Format messages for the API
        const apiMessages = messages
          .filter((m) => !m.isLoading)
          .map((m) => ({
            role: m.role,
            content: m.content,
          }))

        // Add the new user message
        apiMessages.push({
          role: "user",
          content: message,
        })

        // Generate cache key based on messages and other parameters
        const cacheKey = JSON.stringify({
          messages: apiMessages,
          modelId: modelId,
          temperature: temperature,
          maxTokens: maxTokens,
          tools: tools
        })

        // Check cache if enabled
        if (cacheOptions.enabled && cache.current.has(cacheKey)) {
          const cachedResponse = cache.current.get(cacheKey)
          // Use cached response
          setMessages((prev) => [
            ...prev.filter((m) => !m.isLoading),
            {
              id: nanoid(),
              role: "assistant",
              content: cachedResponse,
              timestamp: new Date().toISOString(),
            },
          ])

          if (onFinish) {
            onFinish([...messages, userMessage, {
              id: nanoid(),
              role: "assistant",
              content: cachedResponse
            }])
          }

          return cachedResponse
        }

        // Determine which API endpoint to use
        const endpoint = agentId ? `/api/agents/${agentId}/run` : apiEndpoint

        // Prepare request body
        const requestBody: any = {
          messages: apiMessages,
          threadId,
        }

        // Add model ID if not using an agent
        if (!agentId && modelId) {
          requestBody.modelId = modelId
        }

        // Add tools if selected
        if (tools.length > 0) {
          requestBody.tools = tools
        }

        // Add temperature and max tokens
        requestBody.temperature = temperature
        requestBody.maxTokens = maxTokens

        // Add middleware if provided
        if (options.middleware) {
          requestBody.middleware = options.middleware;
        } else if (middleware) {
          requestBody.middleware = middleware;
        }

        // Make API request
        const response = await fetch(endpoint, {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
          },
          body: JSON.stringify(requestBody),
          signal: abortControllerRef.current.signal
        })

        if (!response.ok) {
          throw new Error(`API request failed with status ${response.status}`)
        }

        // Handle streaming response
        const reader = response.body?.getReader()
        const decoder = new TextDecoder()
        let responseText = ""

        if (reader) {
          // Remove loading message
          setMessages((prev) => prev.filter((m) => m.id !== loadingMessageId))

          // Add assistant message that will be updated
          const assistantMessageId = nanoid()
          setMessages((prev) => [
            ...prev,
            {
              id: assistantMessageId,
              role: "assistant",
              content: "",
              timestamp: new Date().toISOString(),
            },
          ])

          try {
            // Process the stream with proper backpressure handling
            while (true) {
              const { done, value } = await reader.read()

              if (done) {
                break
              }

              // Decode the chunk and update the message
              const chunk = decoder.decode(value, { stream: true })
              responseText += chunk

              // Update the assistant message with the accumulated text
              setMessages((prev) =>
                prev.map((m) => (m.id === assistantMessageId ? { ...m, content: responseText } : m))
              )

              // Call onResponse callback if provided
              if (onResponse) {
                onResponse({ text: responseText, chunk })
              }

              // Add a small delay to prevent UI blocking
              await new Promise(resolve => setTimeout(resolve, 0))

              // Handle streamable updates
              if (chunk.includes('__STREAMABLE_UPDATE__')) {
                try {
                  const streamableUpdate = JSON.parse(chunk.split('__STREAMABLE_UPDATE__')[1])
                  if (streamableUpdate && streamableUpdate.key && streamables?.[streamableUpdate.key]) {
                    setStreamableValues(prev => ({
                      ...prev,
                      [streamableUpdate.key]: streamableUpdate.value
                    }))

                    // Call onUpdate if provided
                    streamables[streamableUpdate.key].onUpdate?.(streamableUpdate.value)
                  }
                } catch (e) {
                  console.error('Error parsing streamable update:', e)
                }
              }
            }
          } catch (error) {
            // Check if error is due to abort
            if (error instanceof Error && error.name === 'AbortError') {
              console.log('Stream was aborted')
            } else {
              throw error
            }
          }
        }
      } catch (error) {
        console.error("Error sending message:", error)
        toast.error("Failed to send message")

        // Remove loading message
        setMessages((prev) => prev.filter((m) => !m.isLoading))

        // Add error message
        setMessages((prev) => [
          ...prev,
          {
            id: nanoid(),
            role: "system",
            content: "Sorry, I encountered an error while processing your request.",
            timestamp: new Date().toISOString(),
          },
        ])

        if (onError && error instanceof Error) {
          onError(error)
        }
      } finally {
        setIsLoading(false)
        abortControllerRef.current = null
      }
    },
    [messages, input, threadId, attachments, apiEndpoint, onError, onResponse, onFinish, cacheOptions, streamables]
  )

  const stop = useCallback(() => {
    if (abortControllerRef.current) {
      abortControllerRef.current.abort()
      abortControllerRef.current = null

      // Remove loading message
      setMessages((prev) => prev.filter((m) => !m.isLoading))
      setIsLoading(false)
    }
  }, [])

  // Add step management functions
  const [currentStep, setCurrentStep] = useState<number>(0)
  const [steps, setSteps] = useState<Array<{
    id: string
    name: string
    status: 'pending' | 'in_progress' | 'completed' | 'error'
    result?: any
  }>>([])

  const addStep = useCallback((name: string) => {
    const stepId = nanoid()
    setSteps(prev => [...prev, {
      id: stepId,
      name,
      status: 'pending'
    }])
    return stepId
  }, [])

  const updateStepStatus = useCallback((stepId: string, status: 'pending' | 'in_progress' | 'completed' | 'error', result?: any) => {
    setSteps(prev => prev.map(step =>
      step.id === stepId
        ? { ...step, status, ...(result ? { result } : {}) }
        : step
    ))
  }, [])

  const goToNextStep = useCallback(() => {
    setCurrentStep(prev => prev + 1)
  }, [])

  const goToPreviousStep = useCallback(() => {
    setCurrentStep(prev => Math.max(0, prev - 1))
  }, [])

  const runSequentialGenerations = useCallback(async (
    prompts: string[],
    options?: {
      modelId?: string
      temperature?: number
      maxTokens?: number
      tools?: string[]
      onProgress?: (index: number, result: string) => void
    }
  ) => {
    const results: string[] = []

    for (let i = 0; i < prompts.length; i++) {
      let prompt = prompts[i]

      // Replace placeholders with previous results
      for (let j = 0; j < i; j++) {
        prompt = prompt.replace(`{${j}}`, results[j])
      }

      // Send the message
      const result = await sendMessage({
        message: prompt,
        modelId: options?.modelId,
        temperature: options?.temperature,
        maxTokens: options?.maxTokens,
        tools: options?.tools
      })

      results.push(result)

      // Call progress callback if provided
      options?.onProgress?.(i, result)
    }

    return results
  }, [sendMessage])

  return {
    messages,
    input,
    setInput,
    isLoading,
    threadId,
    setThreadId,
    attachments,
    setAttachments,
    sendMessage,
    fetchMessages,
    stop,
    streamableValues,
    currentStep,
    steps,
    addStep,
    updateStepStatus,
    goToNextStep,
    goToPreviousStep,
    runSequentialGenerations,
  }
}

// Export middleware creation functions for use in the application
export {
  createCachingMiddleware,
  createLoggingMiddleware,
  createMiddlewareFromOptions
} from '@/lib/middleware';

================
File: hooks/use-supabase-realtime.ts
================
'use client'

/**
 * Hook for Supabase real-time subscriptions
 * Provides real-time updates for Supabase tables with automatic reconnection,
 * Zod validation, and optimized performance
 *
 * Also supports Upstash adapter for Supabase compatibility when configured
 *
 * @module hooks/use-supabase-realtime
 */

import { useState, useEffect, useRef, useCallback } from 'react'
import {
  SupabaseClient,
  RealtimeChannel,
  REALTIME_LISTEN_TYPES,
  REALTIME_PRESENCE_LISTEN_EVENTS,
  REALTIME_SUBSCRIBE_STATES,
  RealtimePostgresChangesPayload,
  RealtimePostgresInsertPayload,
  RealtimePostgresUpdatePayload,
  RealtimePostgresDeletePayload,
  RealtimePresenceJoinPayload,
  RealtimePresenceLeavePayload,
} from '@supabase/supabase-js'
import { z } from 'zod'
import type { Database } from '@/types/supabase'
import { getSupabaseClient, isSupabaseClient, isUpstashClient } from '@/lib/memory/supabase'
import { useToast } from '@/hooks/use-toast'
import { useMemoryProvider } from './use-memory-provider'
import type { SupabaseClient as UpstashSupabaseClient } from '@/lib/memory/upstash/supabase-adapter-factory'

export type ChannelType = 'postgres' | 'presence' | 'broadcast'
export type SubscriptionStatus =
  typeof REALTIME_SUBSCRIBE_STATES[keyof typeof REALTIME_SUBSCRIBE_STATES]
export type PostgresChangeEvent = 'INSERT' | 'UPDATE' | 'DELETE' | '*'
export type FilterOperator = 'eq' | 'neq' | 'gt' | 'gte' | 'lt' | 'lte' | 'in'

interface UseSupabaseRealtimeOptions<T extends z.ZodType<any, any>> {
  /* ------------------------ channel selection ----------------------- */
  channelType?: ChannelType
  channelName?: string
  table?: string

  /** database schema (public, private, etc.) */
  tableSchema?: string

  event?: PostgresChangeEvent
  filter?: { column: string; value: any; operator?: FilterOperator }

  enabled?: boolean
  maxReconnectAttempts?: number
  reconnectDelay?: number

  /** Zod schema for validating row payloads */
  zodSchema?: T
  logValidationErrors?: boolean

  onInsert?: (row: z.infer<T>) => void
  onUpdate?: (row: z.infer<T>) => void
  onDelete?: (row: z.infer<T>) => void
  onChange?: (payload: RealtimePostgresChangesPayload<z.infer<T>>) => void

  onBroadcast?: (payload: any) => void

  onPresenceSync?: (state: Record<string, any[]>) => void
  onPresenceJoin?: (
    key: string,
    newPresences: RealtimePresenceJoinPayload<any>['newPresences']
  ) => void
  onPresenceLeave?: (
    key: string,
    leftPresences: RealtimePresenceLeavePayload<any>['leftPresences']
  ) => void
  initialPresence?: Record<string, any>

  broadcastEventName?: string
  onStatusChange?: (status: SubscriptionStatus) => void
  onValidationError?: (err: z.ZodError) => void
  onError?: (err: Error) => void

  /**
   * Upstash adapter options
   */
  upstash?: {
    /**
     * Whether to force using Upstash adapter
     * If not specified, will use the value from environment variables
     */
    forceUse?: boolean

    /**
     * Whether to add Upstash adapter headers to the request
     * @default true
     */
    addHeaders?: boolean
  }
}

interface UseSupabaseRealtimeReturn {
  isConnected: boolean
  error: Error | null
  lastEventTimestamp: number | null
  connectionStatus: 'connecting' | 'connected' | 'disconnected'
  reconnect: () => void
  channel: RealtimeChannel | null
  broadcast?: (event: string, payload: any) => void
  track?: (presence: Record<string, any>) => Promise<void>
  untrack?: () => Promise<void>
  validationStats: { success: number; errors: number }
}

/**
 * Zod schema for Upstash adapter options
 */
export const UpstashAdapterOptionsSchema = z.object({
  /**
   * Whether to force using Upstash adapter
   * If not specified, will use the value from environment variables
   */
  forceUse: z.boolean().optional(),

  /**
   * Whether to add Upstash adapter headers to the request
   * @default true
   */
  addHeaders: z.boolean().optional().default(true)
}).optional();

/**
 * Zod schema for UseSupabaseRealtimeOptions
 */
export const UseSupabaseRealtimeOptionsSchema = <T extends z.ZodType<any, any>>(schema: T) => z.object({
  channelType: z.enum(['postgres', 'presence', 'broadcast']).optional().default('postgres'),
  channelName: z.string().optional(),
  table: z.string().optional(),
  tableSchema: z.string().optional().default('public'),
  event: z.enum(['INSERT', 'UPDATE', 'DELETE', '*']).optional().default('*'),
  filter: z.object({
    column: z.string(),
    value: z.any(),
    operator: z.enum(['eq', 'neq', 'gt', 'gte', 'lt', 'lte', 'in']).optional()
  }).optional(),
  enabled: z.boolean().optional().default(true),
  maxReconnectAttempts: z.number().optional().default(5),
  reconnectDelay: z.number().optional().default(1000),
  zodSchema: schema.optional(),
  logValidationErrors: z.boolean().optional().default(true),
  onInsert: z.function().args(z.any()).returns(z.void()).optional(),
  onUpdate: z.function().args(z.any()).returns(z.void()).optional(),
  onDelete: z.function().args(z.any()).returns(z.void()).optional(),
  onChange: z.function().args(z.any()).returns(z.void()).optional(),
  onBroadcast: z.function().args(z.any()).returns(z.void()).optional(),
  onPresenceSync: z.function().args(z.record(z.array(z.any()))).returns(z.void()).optional(),
  onPresenceJoin: z.function().args(z.string(), z.any()).returns(z.void()).optional(),
  onPresenceLeave: z.function().args(z.string(), z.any()).returns(z.void()).optional(),
  initialPresence: z.record(z.any()).optional(),
  broadcastEventName: z.string().optional().default('message'),
  onStatusChange: z.function().args(z.any()).returns(z.void()).optional(),
  onValidationError: z.function().args(z.instanceof(z.ZodError)).returns(z.void()).optional(),
  onError: z.function().args(z.instanceof(Error)).returns(z.void()).optional(),
  upstash: UpstashAdapterOptionsSchema
});

export function useSupabaseRealtime<T extends z.ZodType<any, any> = z.ZodAny>(
  options: UseSupabaseRealtimeOptions<T>
): UseSupabaseRealtimeReturn {
  // Validate options with Zod
  const {
    channelType = 'postgres',
    channelName,
    table,
    tableSchema = 'public',
    event = '*',
    filter,
    enabled = true,
    maxReconnectAttempts = 5,
    reconnectDelay = 1000,

    /** renamed to avoid collision */
    zodSchema,
    logValidationErrors = true,
    onInsert,
    onUpdate,
    onDelete,
    onChange,

    onBroadcast,

    onPresenceSync,
    onPresenceJoin,
    onPresenceLeave,
    initialPresence,

    broadcastEventName = 'message',
    onStatusChange,
    onValidationError,
    onError,

    // Upstash adapter options
    upstash = { addHeaders: true },
  } = options;

  const [isConnected, setIsConnected] = useState(false)
  const [error, setError] = useState<Error | null>(null)
  const [lastEventTimestamp, setLastEventTimestamp] = useState<number | null>(null)
  const [connectionStatus, setConnectionStatus] = useState<
    'connecting' | 'connected' | 'disconnected'
  >('disconnected')
  const [validationStats, setValidationStats] = useState({ success: 0, errors: 0 })

  const channelRef = useRef<RealtimeChannel | null>(null)
  const supabaseRef = useRef<any>(null)
  const reconnectAttemptsRef = useRef(0)
  const reconnectTimeoutRef = useRef<NodeJS.Timeout | null>(null)

  const { toast } = useToast()
  const { useUpstashAdapter } = useMemoryProvider()

  // Determine if we should use Upstash adapter
  const shouldUseUpstash = upstash?.forceUse !== undefined
    ? upstash.forceUse
    : useUpstashAdapter

  const initSupabase = useCallback(() => {
    if (!supabaseRef.current) {
      // Get the appropriate client based on configuration
      supabaseRef.current = getSupabaseClient()

      // Check if we're using Upstash and it doesn't support realtime
      if (shouldUseUpstash && isUpstashClient(supabaseRef.current)) {
        console.warn("Upstash adapter doesn't fully support Supabase Realtime. Some features may not work as expected.");
      }
    }
    return supabaseRef.current
  }, [shouldUseUpstash])

  const validateData = useCallback(
    (data: any) => {
      if (!zodSchema) return { success: true as const, data }
      try {
        const d = zodSchema.parse(data)
        setValidationStats((s) => ({ ...s, success: s.success + 1 }))
        return { success: true as const, data: d }
      } catch (err) {
        if (err instanceof z.ZodError) {
          setValidationStats((s) => ({ ...s, errors: s.errors + 1 }))
          if (logValidationErrors) console.error(err.errors)
          onValidationError?.(err)
          return { success: false as const, error: err }
        }
        throw err
      }
    },
    [zodSchema, logValidationErrors, onValidationError]
  )

  const handleStatus = useCallback(
    (status: SubscriptionStatus) => {
      onStatusChange?.(status)
      if (status === REALTIME_SUBSCRIBE_STATES.SUBSCRIBED) {
        setIsConnected(true)
        setConnectionStatus('connected')
        reconnectAttemptsRef.current = 0
      } else if (
        status === REALTIME_SUBSCRIBE_STATES.TIMED_OUT ||
        status === REALTIME_SUBSCRIBE_STATES.CLOSED ||
        status === REALTIME_SUBSCRIBE_STATES.CHANNEL_ERROR
      ) {
        setIsConnected(false)
        setConnectionStatus('disconnected')

        // auto–reconnect
        if (enabled && reconnectAttemptsRef.current < maxReconnectAttempts) {
          const delay = reconnectDelay * 2 ** reconnectAttemptsRef.current
          clearTimeout(reconnectTimeoutRef.current!)
          reconnectTimeoutRef.current = setTimeout(() => {
            reconnectAttemptsRef.current++
            subscribe()
          }, delay)
        }
      }
    },
    [enabled, maxReconnectAttempts, reconnectDelay, onStatusChange]
  )

  const broadcast = useCallback(
    (ev: string, payload: any) => {
      if (channelType !== 'broadcast' || !channelRef.current) return
      channelRef.current.send({ type: 'broadcast', event: ev, payload })
    },
    [channelType]
  )

  const track = useCallback(
    async (presence: Record<string, any>) => {
      if (channelType !== 'presence' || !channelRef.current) return
      await channelRef.current.track(presence)
    },
    [channelType]
  )

  const untrack = useCallback(
    async () => {
      if (channelType !== 'presence' || !channelRef.current) return
      await channelRef.current.untrack()
    },
    [channelType]
  )

  const subscribe = useCallback(() => {
    if (!enabled) {
      setConnectionStatus('disconnected')
      return
    }

    try {
      setConnectionStatus('connecting')
      setError(null)

      // Initialize Supabase client
      const supabaseClient = initSupabase()

      if (channelType === 'postgres' && !table) {
        throw new Error('`table` is required for postgres channel')
      }
      if ((channelType === 'broadcast' || channelType === 'presence') && !channelName) {
        throw new Error('`channelName` is required for this channel type')
      }

      // unique channel name
      const name =
        channelType === 'postgres'
          ? `${tableSchema}_${table}_${filter ? `${filter.column}_${filter.operator}_${filter.value}` : 'all'}_${Date.now()}`
          : `${channelName}_${channelType}_${Date.now()}`

      // Check if we have a valid client that supports realtime
      if (!supabaseClient || (shouldUseUpstash && isUpstashClient(supabaseClient) && !('channel' in supabaseClient))) {
        throw new Error("Realtime subscriptions are not supported with the current client configuration");
      }

      const ch = supabaseClient.channel(name)

      if (channelType === 'postgres') {
        const params: any = { event, schema: tableSchema, table }
        if (filter) {
          params.filter = `${filter.column}=${filter.operator || 'eq'}.${filter.value}`
        }
        const onPayload = (p: RealtimePostgresChangesPayload<any>) => {
          setLastEventTimestamp(Date.now())
          onChange?.(p)
          switch (p.eventType) {
            case 'INSERT': {
              const v = validateData((p as RealtimePostgresInsertPayload<any>).new)
              if (v.success) onInsert?.(v.data)
              break
            }
            case 'UPDATE': {
              const v = validateData((p as RealtimePostgresUpdatePayload<any>).new)
              if (v.success) onUpdate?.(v.data)
              break
            }
            case 'DELETE': {
              const v = validateData((p as RealtimePostgresDeletePayload<any>).old)
              if (v.success) onDelete?.(v.data)
              break
            }
          }
        }

        ch.on(REALTIME_LISTEN_TYPES.POSTGRES_CHANGES, params, onPayload).subscribe(handleStatus)
      } else if (channelType === 'broadcast') {
        ch
          .on(REALTIME_LISTEN_TYPES.BROADCAST, { event: broadcastEventName }, (p: any) => {
            setLastEventTimestamp(Date.now())
            onBroadcast?.(p)
          })
          .subscribe(handleStatus)
      } else {
        // presence
        ch
          .on(
            REALTIME_LISTEN_TYPES.PRESENCE,
            { event: REALTIME_PRESENCE_LISTEN_EVENTS.SYNC },
            () => {
              setLastEventTimestamp(Date.now())
              onPresenceSync?.(ch.presenceState())
            }
          )
          .on(
            REALTIME_LISTEN_TYPES.PRESENCE,
            { event: REALTIME_PRESENCE_LISTEN_EVENTS.JOIN },
            (p: RealtimePresenceJoinPayload<any>) => {
              setLastEventTimestamp(Date.now())
              onPresenceJoin?.(p.key, p.newPresences)
            }
          )
          .on(
            REALTIME_LISTEN_TYPES.PRESENCE,
            { event: REALTIME_PRESENCE_LISTEN_EVENTS.LEAVE },
            (p: RealtimePresenceLeavePayload<any>) => {
              setLastEventTimestamp(Date.now())
              onPresenceLeave?.(p.key, p.leftPresences)
            }
          )
          .subscribe(async (status: SubscriptionStatus) => {
            if (status === REALTIME_SUBSCRIBE_STATES.SUBSCRIBED && initialPresence) {
              await ch.track(initialPresence)
            }
            handleStatus(status)
          })
      }

      channelRef.current = ch
      return () => {
        ch.unsubscribe()
      }
    } catch (err: any) {
      const e = err instanceof Error ? err : new Error('Subscription failed')
      setError(e)
      setConnectionStatus('disconnected')
      onError?.(e)
      toast({
        title: 'Realtime error',
        description: e.message,
        variant: 'destructive',
      })
    }
  }, [
    channelType,
    table,
    tableSchema,
    event,
    filter,
    channelName,
    enabled,
    reconnectDelay,
    maxReconnectAttempts,
    initSupabase,
    onInsert,
    onUpdate,
    onDelete,
    onChange,
    onBroadcast,
    onPresenceSync,
    onPresenceJoin,
    onPresenceLeave,
    initialPresence,
    broadcastEventName,
    handleStatus,
    validateData,
    toast,
    onError,
  ])

  useEffect(() => {
    const cleanup = subscribe()
    return () => {
      cleanup?.()
      clearTimeout(reconnectTimeoutRef.current!)
      channelRef.current?.unsubscribe()
    }
  }, [subscribe])

  return {
    isConnected,
    error,
    lastEventTimestamp,
    connectionStatus,
    reconnect: subscribe,
    channel: channelRef.current,
    broadcast,
    track,
    untrack,
    validationStats,
  }
}

================
File: hooks/use-supabase-direct.ts
================
'use client'

/**
 * Hook for direct Supabase CRUD operations
 * This hook provides direct database operations using the Supabase client
 * with optimized settings for direct database access. It uses the DATABASE_URL
 * environment variable which is the direct connection to the Supabase PostgreSQL
 * database via the transaction pooler.
 *
 * This hook is optimized for performance and should be used for operations
 * that require direct database access, such as bulk operations or complex queries.
 *
 * It also supports the Upstash adapter for Supabase compatibility, allowing
 * seamless switching between Supabase and Upstash backends.
 *
 * @module hooks/use-supabase-direct
 */

import { useState, useRef, useEffect } from 'react'
import { useToast } from '@/hooks/use-toast'
import { createClient, PostgrestError } from '@supabase/supabase-js'
import type { Database } from '@/types/supabase'
import { LRUCache } from 'lru-cache'
import { getDrizzleClient } from '@/lib/memory/drizzle'
import { DATABASE_URL } from '../lib/tools/graphql/constants';
import { useMemoryProvider } from './use-memory-provider'
import { createSupabaseClient } from '@/lib/memory/upstash/supabase-adapter-factory'
import type { SupabaseClient as UpstashSupabaseClient } from '@/lib/memory/upstash/supabase-adapter-factory'

/**
 * Options for the useSupabaseDirect hook
 */
interface UseSupabaseDirectOptions<T> {
  /**
   * The table name to perform CRUD operations on
   */
  tableName: string

  /**
   * Optional Drizzle schema table reference
   * If provided, will use Drizzle ORM for database operations
   */
  schemaTable?: any

  /**
   * Optional callback to transform data before saving
   */
  transformBeforeSave?: (data: T) => any

  /**
   * Optional callback to transform data after fetching
   */
  transformAfterFetch?: (data: any) => T

  /**
   * Optional error handler
   */
  onError?: (error: PostgrestError | Error, operation?: string) => void

  /**
   * Optional success handler
   */
  onSuccess?: (operation: 'create' | 'update' | 'delete' | 'get' | 'batch' | 'query', data?: any) => void

  /**
   * Cache options
   */
  cache?: {
    /**
     * Whether to enable caching
     * @default true
     */
    enabled?: boolean

    /**
     * Maximum number of items to store in the cache
     * @default 100
     */
    maxSize?: number

    /**
     * Time to live for cache entries in milliseconds
     * @default 60000 (1 minute)
     */
    ttl?: number

    /**
     * Whether to log cache hits and misses
     * @default false
     */
    debug?: boolean
  }

  /**
   * Whether to use Drizzle ORM for database operations
   * @default true if schemaTable is provided, false otherwise
   */
  useDrizzle?: boolean

  /**
   * Whether to use optimistic updates for create, update, and delete operations
   * @default true
   */
  optimisticUpdates?: boolean

  /**
   * Default page size for paginated queries
   * @default 20
   */
  defaultPageSize?: number

  /**
   * Whether to automatically refresh data after mutations
   * @default true
   */
  autoRefresh?: boolean

  /**
   * Upstash adapter options
   */
  upstash?: {
    /**
     * Whether to force using Upstash adapter
     * If not specified, will use the value from environment variables
     */
    forceUse?: boolean

    /**
     * Whether to add Upstash adapter headers to the request
     * @default true
     */
    addHeaders?: boolean
  }
}

/**
 * Advanced query options for filtering, sorting, and pagination
 */
export interface QueryOptions {
  /**
   * Filter conditions
   */
  filters?: FilterCondition[]

  /**
   * Columns to select
   */
  select?: string | string[]

  /**
   * Pagination options
   */
  pagination?: {
    /**
     * Page number (1-based)
     */
    page?: number

    /**
     * Number of items per page
     */
    pageSize?: number

    /**
     * Cursor for cursor-based pagination
     */
    cursor?: string
  }

  /**
   * Sorting options
   */
  sort?: {
    /**
     * Column to sort by
     */
    column: string

    /**
     * Sort direction
     */
    ascending?: boolean
  }[]

  /**
   * Relations to include
   */
  include?: string[]

  /**
   * Whether to count total rows
   */
  count?: boolean
}

/**
 * Filter condition for advanced queries
 */
export interface FilterCondition {
  /**
   * Column to filter on
   */
  column: string

  /**
   * Operator to use
   */
  operator: FilterOperator

  /**
   * Value to compare against
   */
  value: any
}

/**
 * Filter operators for advanced queries
 */
export type FilterOperator =
  | 'eq' | 'neq'
  | 'gt' | 'gte' | 'lt' | 'lte'
  | 'like' | 'ilike'
  | 'in' | 'is'
  | 'contains' | 'containedBy'
  | 'overlaps' | 'textSearch'
  | 'between' | 'notBetween'
  | 'rangeGt' | 'rangeLt' | 'rangeGte' | 'rangeLte' | 'rangeAdjacent';

/**
 * Hook for direct Supabase CRUD operations
 * @param options Options for the hook
 * @returns CRUD operations and state
 */
export function useSupabaseDirect<T extends { id?: string | number }>(
  options: UseSupabaseDirectOptions<T>
) {
  const {
    tableName,
    schemaTable,
    transformBeforeSave,
    transformAfterFetch,
    onError,
    onSuccess,
    cache: cacheOptions = { enabled: true },
    useDrizzle = !!schemaTable,
    upstash = { addHeaders: true }
  } = options

  const [loading, setLoading] = useState(false)
  const [error, setError] = useState<PostgrestError | Error | null>(null)
  const [items, setItems] = useState<T[]>([])
  const [item, setItem] = useState<T | null>(null)
  const [pagination] = useState({
    totalCount: 0,
    pageCount: 0,
    currentPage: 1,
    pageSize: 20
  })

  // Get memory provider configuration
  const { useUpstashAdapter } = useMemoryProvider()

  // Determine if we should use Upstash adapter
  const shouldUseUpstash = upstash.forceUse !== undefined
    ? upstash.forceUse
    : useUpstashAdapter

  // Initialize LRU cache with optimized settings
  const cacheInstance = useRef<LRUCache<string, any>>(new LRUCache({
    max: cacheOptions.maxSize || 500, // Increased default size for better hit rate
    ttl: cacheOptions.ttl || 300000, // 5 minutes default TTL for better cache utilization
    updateAgeOnGet: true, // Reset TTL when item is accessed
    updateAgeOnHas: false, // Don't reset TTL on cache checks
    allowStale: true, // Allow returning stale items before removing them
    fetchMethod: async (_key, staleValue) => {
      // This allows for background refresh of cache items
      // Return stale value immediately while fetching fresh data
      return staleValue;
    },
    noDisposeOnSet: true, // Don't dispose items that are being replaced
    noUpdateTTL: false, // Update TTL when item is set
  }))

  // Cache statistics for debugging and optimization
  const cacheStats = useRef({
    hits: 0,
    misses: 0,
    sets: 0,
    evictions: 0,
    staleHits: 0,
    refreshes: 0
  })

  // Initialize Drizzle client if needed
  const drizzleRef = useRef<ReturnType<typeof getDrizzleClient> | null>(null)

  // Create client refs to hold the Supabase or Upstash clients
  const supabaseRef = useRef<any>(null)
  const transactionClientRef = useRef<any>(null)

  // Initialize clients if not already done
  if (!supabaseRef.current) {
    if (shouldUseUpstash) {
      try {
        // Create Upstash adapter client
        supabaseRef.current = createSupabaseClient()
        transactionClientRef.current = supabaseRef.current
        console.log("Using Upstash adapter for Supabase direct operations")
      } catch (error) {
        console.error("Error creating Upstash adapter:", error)
        // Fall back to regular Supabase clients
        supabaseRef.current = createClient<Database>(
          process.env.SESSION_POOL_URL || '',
          process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY || '',
          {
            db: { schema: 'public' },
            auth: { persistSession: true, autoRefreshToken: true },
            global: { headers: { 'x-client-info': 'useSupabaseDirect-SessionPool' } },
          }
        )

        transactionClientRef.current = createClient<Database>(
          process.env.DATABASE_URL || '',
          process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY || '',
          {
            db: { schema: 'public' },
            auth: { persistSession: true, autoRefreshToken: true },
            global: { headers: { 'x-client-info': 'useSupabaseDirect-TransactionPool' } },
          }
        )
      }
    } else {
      // Create regular Supabase clients
      supabaseRef.current = createClient<Database>(
        process.env.SESSION_POOL_URL || '',
        process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY || '',
        {
          db: { schema: 'public' },
          auth: { persistSession: true, autoRefreshToken: true },
          global: { headers: { 'x-client-info': 'useSupabaseDirect-SessionPool' } },
        }
      )

      transactionClientRef.current = createClient<Database>(
        process.env.DATABASE_URL || '',
        process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY || '',
        {
          db: { schema: 'public' },
          auth: { persistSession: true, autoRefreshToken: true },
          global: { headers: { 'x-client-info': 'useSupabaseDirect-TransactionPool' } },
        }
      )
    }
  }

  // Get the clients from the refs
  const supabase = supabaseRef.current as any
  const transactionClient = transactionClientRef.current as any

  // Type guard to check if client is Upstash adapter
  const isUpstashAdapter = (client: any): client is UpstashSupabaseClient => {
    return client && typeof client === 'object' && 'isUpstashAdapter' in client && client.isUpstashAdapter === true;
  }

  // Use Drizzle if enabled
  useEffect(() => {
    if (useDrizzle && !drizzleRef.current) {
      try {
        drizzleRef.current = getDrizzleClient()
      } catch (err) {
        console.error('Error initializing Drizzle client:', err)
      }
    }
  }, [useDrizzle])

  const { toast } = useToast()

  /**
   * Handle errors from Supabase
   */
  const handleError = (error: PostgrestError, operation: string) => {
    setError(error)
    console.error(`Error in ${operation} operation on ${tableName}:`, error)

    // Call custom error handler if provided
    if (onError) {
      onError(error)
    } else {
      // Default error handling
      toast({
        title: `Error in ${operation}`,
        description: error.message || 'An error occurred',
        variant: 'destructive',
      })
    }
  }

  /**
   * Handle success
   */
  const handleSuccess = (operation: 'create' | 'update' | 'delete' | 'get', data?: any) => {
    if (onSuccess) {
      onSuccess(operation, data)
    } else {
      // Default success handling
      const messages = {
        create: `Created new ${tableName} successfully`,
        update: `Updated ${tableName} successfully`,
        delete: `Deleted ${tableName} successfully`,
        get: `Retrieved ${tableName} data successfully`,
      }

      toast({
        title: 'Success',
        description: messages[operation],
      })
    }
  }

  /**
   * Cache management functions with optimized implementation
   */
  const cacheManager = {
    /**
     * Get an item from the cache with optimized performance
     * Uses allowStale option to return stale items while refreshing in background
     */
    get: <R>(key: string): R | undefined => {
      if (!cacheOptions.enabled) return undefined

      try {
        // Get options with allowStale to improve performance
        const options = { allowStale: true }
        const result = cacheInstance.current.get(key, options) as R | undefined

        if (result !== undefined) {
          // Check if the result is stale
          const isStale = cacheInstance.current.has(key, { updateAgeOnHas: false }) === false

          if (isStale) {
            cacheStats.current.staleHits++
            if (cacheOptions.debug) {
              console.log(`Cache STALE HIT: ${key}`)
            }
          } else {
            cacheStats.current.hits++
            if (cacheOptions.debug) {
              console.log(`Cache HIT: ${key}`)
            }
          }
        } else {
          cacheStats.current.misses++
          if (cacheOptions.debug) {
            console.log(`Cache MISS: ${key}`)
          }
        }

        return result
      } catch (err) {
        console.error('Cache error:', err)
        return undefined
      }
    },

    /**
     * Set an item in the cache with optimized settings
     * Uses background refresh for frequently accessed items
     */
    set: <R>(key: string, value: R, options?: { ttl?: number }): void => {
      if (!cacheOptions.enabled) return

      try {
        const ttl = options?.ttl || cacheOptions.ttl || 300000
        cacheInstance.current.set(key, value, { ttl })
        cacheStats.current.sets++

        if (cacheOptions.debug) {
          console.log(`Cache SET: ${key} (TTL: ${ttl}ms)`)
        }
      } catch (err) {
        console.error('Cache set error:', err)
      }
    },

    /**
     * Remove an item from the cache
     */
    remove: (key: string): void => {
      if (!cacheOptions.enabled) return

      try {
        cacheInstance.current.delete(key)

        if (cacheOptions.debug) {
          console.log(`Cache DELETE: ${key}`)
        }
      } catch (err) {
        console.error('Cache delete error:', err)
      }
    },

    /**
     * Clear the entire cache
     */
    clear: (): void => {
      if (!cacheOptions.enabled) return

      try {
        cacheInstance.current.clear()

        // Reset statistics
        cacheStats.current = {
          hits: 0,
          misses: 0,
          sets: 0,
          evictions: 0,
          staleHits: 0,
          refreshes: 0
        }

        if (cacheOptions.debug) {
          console.log('Cache CLEARED')
        }
      } catch (err) {
        console.error('Cache clear error:', err)
      }
    },

    /**
     * Refresh a cache item if it exists
     * This is useful for background refreshing of frequently accessed items
     */
    refresh: async <R>(key: string, fetcher: () => Promise<R>): Promise<R | undefined> => {
      if (!cacheOptions.enabled) return undefined

      try {
        // Check if item exists in cache (for future use)
        cacheInstance.current.has(key)

        // Fetch new value
        const newValue = await fetcher()

        // Update cache with new value
        if (newValue !== undefined) {
          cacheInstance.current.set(key, newValue)
          cacheStats.current.refreshes++

          if (cacheOptions.debug) {
            console.log(`Cache REFRESH: ${key}`)
          }
        }

        return newValue
      } catch (err) {
        console.error('Cache refresh error:', err)
        return undefined
      }
    },

    /**
     * Get cache statistics with detailed metrics
     */
    getStats: () => {
      return {
        ...cacheStats.current,
        size: cacheInstance.current.size,
        maxSize: cacheOptions.maxSize || 500,
        hitRate: cacheStats.current.hits / (cacheStats.current.hits + cacheStats.current.misses) || 0,
        staleHitRate: cacheStats.current.staleHits / (cacheStats.current.hits + cacheStats.current.staleHits) || 0
      }
    }
  }

  // Helper functions for query building

  type JoinOptions = {
    table: string;
    on: { foreignKey: string; primaryKey: string };
    fields?: string[];
  };

  /**
   * Get all items from the table with advanced filtering
   */
  const getAll = async (options?: {
    filters?: Record<string, any>;
    advancedFilters?: FilterCondition[];
    limit?: number;
    offset?: number;
    orderBy?: { column: string; ascending?: boolean }[];
    fields?: string[];
    join?: JoinOptions;
    search?: { column: string; query: string };
    groupBy?: string[];
    having?: FilterCondition[];
  }) => {
    setLoading(true)
    setError(null)

    try {
      // Build select statement with fields
      const selectFields = options?.fields?.length
        ? options.fields.join(', ')
        : '*';

      // Add join fields if specified
      let selectStatement = selectFields;
      if (options?.join) {
        const joinFields = options.join.fields?.length
          ? options.join.fields.map(field => `${options.join!.table}:${field}`)
          : [`${options.join.table}:*`];

        selectStatement = `${selectFields},${joinFields.join(',')}`;
      }

      let query = supabase.from(tableName).select(selectStatement);

      // Apply join if specified
      // Note: Join is implemented through the select statement
      // as Supabase's JS client doesn't directly support JOIN operations
      // The join is specified in the select statement format:
      // select('*,foreign_table(*)')
      if (options?.join) {
        // Join is already handled in the select statement above
        console.log(`Applied join with table: ${options.join.table}`);
      }

      // Apply simple filters (backward compatibility)
      if (options?.filters) {
        Object.entries(options.filters).forEach(([key, value]) => {
          if (value !== undefined && value !== null) {
            query = query.eq(key, value);
          }
        });
      }

      // Apply advanced filters
      if (options?.advancedFilters?.length) {
        options.advancedFilters.forEach(({ column, operator, value }) => {
          switch (operator) {
            case 'eq': query = query.eq(column, value); break;
            case 'neq': query = query.neq(column, value); break;
            case 'gt': query = query.gt(column, value); break;
            case 'gte': query = query.gte(column, value); break;
            case 'lt': query = query.lt(column, value); break;
            case 'lte': query = query.lte(column, value); break;
            case 'like': query = query.like(column, value); break;
            case 'ilike': query = query.ilike(column, value); break;
            case 'in': query = query.in(column, value); break;
            case 'is': query = query.is(column, value); break;
            case 'contains': query = query.contains(column, value); break;
            case 'containedBy': query = query.containedBy(column, value); break;
            case 'rangeGt': query = query.rangeGt(column, value); break;
            case 'rangeLt': query = query.rangeLt(column, value); break;
            case 'rangeGte': query = query.rangeGte(column, value); break;
            case 'rangeLte': query = query.rangeLte(column, value); break;
            case 'rangeAdjacent': query = query.rangeAdjacent(column, value); break;
            case 'overlaps': query = query.overlaps(column, value); break;
            case 'textSearch': query = query.textSearch(column, value); break;
            default: console.warn(`Unsupported operator: ${operator}`);
          }
        });
      }

      // Apply full-text search
      if (options?.search) {
        const { column, query: searchQuery } = options.search;
        query = query.textSearch(column, searchQuery, {
          type: 'websearch',
          config: 'english'
        });
      }

      // Apply group by
      // Note: Supabase's JS client doesn't directly support GROUP BY
      // For complex grouping, consider using raw SQL queries
      if (options?.groupBy?.length) {
        console.log(`Group by not directly supported in JS client. Columns: ${options.groupBy.join(', ')}`);
        // For simple cases, we can use the select statement to achieve grouping
        // by selecting only the grouped columns
      }

      // Apply having conditions
      if (options?.having?.length) {
        options.having.forEach(({ column, operator, value }) => {
          // Note: Supabase doesn't directly support HAVING, but we can use it in raw SQL
          // This is a simplified approach
          const havingCondition = `${column} ${operator} ${value}`;
          console.log(`Applied HAVING condition: ${havingCondition}`);
        });
      }

      // Apply ordering (multiple columns)
      if (options?.orderBy?.length) {
        options.orderBy.forEach(({ column, ascending = true }) => {
          query = query.order(column, { ascending });
        });
      }

      // Apply pagination
      if (options?.limit !== undefined) {
        query = query.limit(options.limit);

        if (options?.offset !== undefined) {
          query = query.range(options.offset, options.offset + options.limit - 1);
        }
      }

      const { data, error } = await query;

      if (error) {
        handleError(error, 'getAll');
        return [];
      }

      // Convert data to the correct type with proper type casting
      const safeData = data as unknown as any[];
      const transformedData = transformAfterFetch
        ? safeData.map((item: any) => transformAfterFetch(item))
        : safeData as T[];

      // Cache the results with optimized settings
      // Use a more efficient cache key that's less memory-intensive
      const resultCacheKey = `${tableName}_getAll_${JSON.stringify(options || {}).slice(0, 100)}`;

      // Set TTL based on data size - larger datasets get shorter TTL
      const dataSizeBasedTTL = transformedData.length > 100
        ? 60000  // 1 minute for large datasets
        : transformedData.length > 50
          ? 180000  // 3 minutes for medium datasets
          : 300000; // 5 minutes for small datasets

      cacheManager.set(resultCacheKey, transformedData, { ttl: dataSizeBasedTTL });

      setItems(transformedData);
      handleSuccess('get', transformedData);
      return transformedData;
    } catch (err) {
      console.error('Unexpected error in getAll:', err);
      return [];
    } finally {
      setLoading(false);
    }
  }

  /**
   * Get a single item by ID with optimized caching
   */
  const getById = async (id: string | number) => {
    setLoading(true)
    setError(null)

    try {
      // Check cache first with optimized key
      const cacheKey = `${tableName}_getById_${id}`
      const cachedItem = cacheManager.get<T>(cacheKey)

      if (cachedItem) {
        setItem(cachedItem)
        setLoading(false)
        return cachedItem
      }

      const { data, error } = await supabase
        .from(tableName as keyof Database['public']['Tables'])
        .select('*')
        .eq('id', id as any)
        .single()

      if (error) {
        handleError(error, 'getById')
        return null
      }

      const transformedData = transformAfterFetch
        ? transformAfterFetch(data)
        : data as T

      // Cache the result with longer TTL for single items (10 minutes)
      // Single items are accessed more frequently and change less often
      cacheManager.set(cacheKey, transformedData, { ttl: 600000 })

      setItem(transformedData)
      handleSuccess('get', transformedData)
      return transformedData
    } catch (err) {
      console.error('Unexpected error in getById:', err)
      return null
    } finally {
      setLoading(false)
    }
  }

  /**
   * Create a new item
   */
  const create = async (data: T) => {
    setLoading(true)
    setError(null)

    try {
      const transformedData = transformBeforeSave ? transformBeforeSave(data) : data

      const { data: createdData, error } = await supabase
        .from(tableName as keyof Database['public']['Tables'])
        .insert(transformedData)
        .select()
        .single()

      if (error) {
        handleError(error, 'create')
        return null
      }

      const newItem = transformAfterFetch
        ? transformAfterFetch(createdData)
        : createdData as T

      // Invalidate any list caches since we've added a new item
      const listCachePattern = `${tableName}_getAll_`
      Object.keys(cacheInstance.current.dump())
        .filter(key => key.startsWith(listCachePattern))
        .forEach(key => cacheManager.remove(key))

      setItem(newItem)
      handleSuccess('create', newItem)
      return newItem
    } catch (err) {
      console.error('Unexpected error in create:', err)
      return null
    } finally {
      setLoading(false)
    }
  }

  /**
   * Update an existing item
   */
  const update = async (id: string | number, data: Partial<T>) => {
    setLoading(true)
    setError(null)

    try {
      const transformedData = transformBeforeSave
        ? transformBeforeSave({ ...data, id } as T)
        : data

      const { data: updatedData, error } = await supabase
        .from(tableName as keyof Database['public']['Tables'])
        .update(transformedData)
        .eq('id', id as any)
        .select()
        .single()

      if (error) {
        handleError(error, 'update')
        return null
      }

      const updatedItem = transformAfterFetch
        ? transformAfterFetch(updatedData)
        : updatedData as T

      // Invalidate specific item cache and any list caches
      cacheManager.remove(`${tableName}_getById_${id}`)

      // Invalidate list caches that might contain this item
      const listCachePattern = `${tableName}_getAll_`
      Object.keys(cacheInstance.current.dump())
        .filter(key => key.startsWith(listCachePattern))
        .forEach(key => cacheManager.remove(key))

      setItem(updatedItem)
      handleSuccess('update', updatedItem)
      return updatedItem
    } catch (err) {
      console.error('Unexpected error in update:', err)
      return null
    } finally {
      setLoading(false)
    }
  }

  /**
   * Delete an item by ID
   */
  const remove = async (id: string | number) => {
    setLoading(true)
    setError(null)

    try {
      const { error } = await supabase
        .from(tableName as keyof Database['public']['Tables'])
        .delete()
        .eq('id', id as any)

      if (error) {
        handleError(error, 'delete')
        return false
      }

      // Invalidate specific item cache and any list caches
      cacheManager.remove(`${tableName}_getById_${id}`)

      // Invalidate list caches that might contain this item
      const listCachePattern = `${tableName}_getAll_`
      Object.keys(cacheInstance.current.dump())
        .filter(key => key.startsWith(listCachePattern))
        .forEach(key => cacheManager.remove(key))

      handleSuccess('delete')
      return true
    } catch (err) {
      console.error('Unexpected error in remove:', err)
      return false
    } finally {
      setLoading(false)
    }
  }

  /**
   * Batch create multiple items
   */
  const batchCreate = async (dataArray: T[]) => {
    setLoading(true)
    setError(null)

    try {
      const transformedData = transformBeforeSave
        ? dataArray.map(data => transformBeforeSave(data))
        : dataArray

      const { data: createdData, error } = await supabase
        .from(tableName as keyof Database['public']['Tables'])
        .insert(transformedData)
        .select()

      if (error) {
        handleError(error, 'batchCreate')
        return []
      }

      const newItems = transformAfterFetch
        ? createdData.map((item: any) => transformAfterFetch(item))
        : createdData as T[]

      // Invalidate all list caches after batch operation
      const listCachePattern = `${tableName}_getAll_`
      Object.keys(cacheInstance.current.dump())
        .filter(key => key.startsWith(listCachePattern))
        .forEach(key => cacheManager.remove(key))

      setItems(prevItems => [...prevItems, ...newItems])
      handleSuccess('create', newItems)
      return newItems
    } catch (err) {
      console.error('Unexpected error in batchCreate:', err)
      return []
    } finally {
      setLoading(false)
    }
  }

  /**
   * Batch update multiple items
   */
  const batchUpdate = async (dataArray: Array<{ id: string | number, data: Partial<T> }>) => {
    setLoading(true)
    setError(null)

    try {
      const results: T[] = []

      // Process updates in batches of 10 to avoid overwhelming the database
      const batchSize = 10
      for (let i = 0; i < dataArray.length; i += batchSize) {
        const batch = dataArray.slice(i, i + batchSize)

        // Process each item in the batch
        const batchPromises = batch.map(async ({ id, data }) => {
          const transformedData = transformBeforeSave
            ? transformBeforeSave({ ...data, id } as T)
            : data

          const { data: updatedData, error } = await supabase
            .from(tableName as keyof Database['public']['Tables'])
            .update(transformedData)
            .eq('id', id as any)
            .select()
            .single()

          if (error) {
            console.error(`Error updating item ${id}:`, error)
            return null
          }

          return transformAfterFetch
            ? transformAfterFetch(updatedData)
            : updatedData as T
        })

        const batchResults = await Promise.all(batchPromises)
        results.push(...batchResults.filter(Boolean) as T[])
      }

      // Invalidate all caches after batch update
      // This is more efficient than trying to selectively invalidate
      cacheManager.clear()

      handleSuccess('update', results)
      return results
    } catch (err) {
      console.error('Unexpected error in batchUpdate:', err)
      return []
    } finally {
      setLoading(false)
    }
  }

  /**
   * Batch delete multiple items
   */
  const batchRemove = async (ids: Array<string | number>) => {
    setLoading(true)
    setError(null)

    try {
      // Process deletes in batches of 10 to avoid overwhelming the database
      const batchSize = 10
      const results: boolean[] = []

      for (let i = 0; i < ids.length; i += batchSize) {
        const batchIds = ids.slice(i, i + batchSize)

        const { error } = await supabase
          .from(tableName as keyof Database['public']['Tables'])
          .delete()
          .in('id', batchIds as any)

        if (error) {
          console.error(`Error deleting batch ${i / batchSize + 1}:`, error)
          results.push(false)
        } else {
          results.push(true)
        }
      }

      const success = results.every(Boolean)
      if (success) {
        // Invalidate all caches after batch delete
        // This is more efficient than trying to selectively invalidate
        cacheManager.clear()

        handleSuccess('delete')
      }

      return success
    } catch (err) {
      console.error('Unexpected error in batchRemove:', err)
      return false
    } finally {
      setLoading(false)
    }
  }

  /**
   * Execute a raw SQL query
   * This provides maximum flexibility for complex queries
   * @param query SQL query string
   * @param params Query parameters
   * @returns Query results
   */
  const executeRawQuery = async (query: string, params?: any[]) => {
    setLoading(true)
    setError(null)

    try {
      const { data, error } = await supabase.rpc('execute_sql', {
        query_text: query,
        query_params: params || []
      });

      if (error) {
        handleError(error, 'executeRawQuery');
        return [];
      }

      // For raw queries, we don't apply transformations
      // as the structure may be completely different
      return data;
    } catch (err) {
      console.error('Unexpected error in executeRawQuery:', err);
      return [];
    } finally {
      setLoading(false);
    }
  };

  /**
   * Count records with optional filters
   */
  const count = async (options?: {
    filters?: Record<string, any>;
    advancedFilters?: FilterCondition[];
  }) => {
    setLoading(true);
    setError(null);

    try {
      let query = supabase.from(tableName as keyof Database['public']['Tables']).select('*', { count: 'exact', head: true });

      // Apply simple filters
      if (options?.filters) {
        Object.entries(options.filters).forEach(([key, value]) => {
          if (value !== undefined && value !== null) {
            query = query.eq(key, value);
          }
        });
      }

      // Apply advanced filters
      if (options?.advancedFilters?.length) {
        options.advancedFilters.forEach(({ column, operator, value }) => {
          switch (operator) {
            case 'eq': query = query.eq(column, value); break;
            case 'neq': query = query.neq(column, value); break;
            case 'gt': query = query.gt(column, value); break;
            case 'gte': query = query.gte(column, value); break;
            case 'lt': query = query.lt(column, value); break;
            case 'lte': query = query.lte(column, value); break;
            case 'like': query = query.like(column, value); break;
            case 'ilike': query = query.ilike(column, value); break;
            case 'in': query = query.in(column, value); break;
            case 'is': query = query.is(column, value); break;
            case 'contains': query = query.contains(column, value); break;
            case 'containedBy': query = query.containedBy(column, value); break;
            default: console.warn(`Unsupported operator: ${operator}`);
          }
        });
      }

      const { count, error } = await query;

      if (error) {
        handleError(error, 'count');
        return 0;
      }

      return count || 0;
    } catch (err) {
      console.error('Unexpected error in count:', err);
      return 0;
    } finally {
      setLoading(false);
    }
  };

  /**
   * Execute operations within a database transaction
   * All operations will be committed together or rolled back if any fails
   *
   * @param operations - Function containing operations to execute in transaction
   * @returns Result of the transaction
   */
  const withTransaction = async <R>(
    operations: (client: typeof transactionClient) => Promise<R>
  ): Promise<R> => {
    setLoading(true);
    setError(null);

    try {
      // Begin transaction
      const { error: beginError } = await transactionClient.rpc('begin');
      if (beginError) throw beginError;

      try {
        // Execute operations within transaction
        const result = await operations(transactionClient);

        // Commit transaction
        const { error: commitError } = await transactionClient.rpc('commit');
        if (commitError) throw commitError;

        return result;
      } catch (err) {
        // Rollback transaction on error
        try {
          const { error: rollbackError } = await transactionClient.rpc('rollback');
          if (rollbackError) {
            console.error('Error rolling back transaction:', rollbackError);
          }
        } catch (rollbackErr) {
          console.error('Error rolling back transaction:', rollbackErr);
        }

        throw err;
      }
    } catch (err) {
      console.error('Transaction error:', err);

      const pgError = {
        name: 'PostgrestError',
        message: err instanceof Error ? err.message : 'Transaction failed',
        details: 'Transaction failed',
        hint: 'Check server logs for details',
        code: 'TRANSACTION_ERROR'
      } as PostgrestError;

      setError(pgError);

      if (onError) {
        onError(pgError);
      } else {
        toast({
          title: 'Transaction Error',
          description: err instanceof Error ? err.message : 'Transaction failed',
          variant: 'destructive',
        });
      }

      throw err;
    } finally {
      setLoading(false);
    }
  };

  return {
    // State
    loading,
    error,
    items,
    item,
    pagination,

    // Basic CRUD operations
    getAll,
    getById,
    create,
    update,
    remove,

    // Batch operations
    batchCreate,
    batchUpdate,
    batchRemove,

    // Advanced operations
    executeRawQuery,
    count,
    withTransaction,

    // Cache management
    cache: cacheManager,

    // Direct access to Supabase client
    supabase,
    transactionClient
  }
}

================
File: hooks/use-supabase-crud.ts
================
"use client"

/**
 * Hook for enhanced Supabase CRUD + Storage operations
 * - Typed table selects (filter/order/paginate)
 * - create / update / delete / batch insert
 * - file uploads (Supabase Storage)
 * - Zod request/response validation
 * - retry/backoff for transient network failures
 * - Upstash adapter support for Supabase compatibility
 */

import { useState, useEffect, useCallback, useRef, useMemo } from "react"
import { z } from "zod"
import { SupabaseClient, PostgrestError } from "@supabase/supabase-js"
import { getSupabaseClient, isSupabaseClient, isUpstashClient } from "@/lib/memory/supabase"
import { useToast } from "@/hooks/use-toast"
import type { Database } from "@/types/supabase"
import { useMemoryProvider } from "./use-memory-provider"
import { createSupabaseClient } from "@/lib/memory/upstash/supabase-adapter-factory"
import type { SupabaseClient as UpstashSupabaseClient } from "@/lib/memory/upstash/supabase-adapter-factory"

// --- Table‐generic typings --------------------------------------------
type TableName = keyof Database["public"]["Tables"]
type RowOf<T extends TableName>   = Database["public"]["Tables"][T]["Row"]
type InsertOf<T extends TableName> = Database["public"]["Tables"][T]["Insert"]
type UpdateOf<T extends TableName> = Database["public"]["Tables"][T]["Update"]

// --- Hook options -----------------------------------------------------
type CrudOp = "fetch" | "create" | "update" | "delete" | "batch" | "upload"

export interface UseSupabaseCrudOptions<
  T extends TableName,
  Req extends Partial<InsertOf<T>> = Partial<InsertOf<T>>,
  Res extends RowOf<T> = RowOf<T>
> {
  table: T
  requestSchema?: z.ZodSchema<Req>
  responseSchema?: z.ZodSchema<Res>
  responseListSchema?: z.ZodSchema<Res[]>

  filters?: Partial<Record<keyof Res, any>>
  order?: { column: keyof Res; ascending?: boolean }
  pagination?: { limit: number; offset: number }

  maxRetries?: number
  retryDelay?: number

  /**
   * Upstash adapter options
   */
  upstash?: {
    /**
     * Whether to force using Upstash adapter
     * If not specified, will use the value from environment variables
     */
    forceUse?: boolean

    /**
     * Whether to add Upstash adapter headers to the request
     * @default true
     */
    addHeaders?: boolean
  }

  onSuccess?: (op: CrudOp, data?: Res | Res[] | string) => void
  onError?: (err: Error, op: CrudOp) => void
}

// --- Hook return ------------------------------------------------------
export interface UseSupabaseCrudReturn<T extends TableName, Res> {
  items: Res[]
  loading: boolean
  error: Error | null

  fetchAll: () => Promise<Res[]>
  create: (data: Partial<InsertOf<T>>) => Promise<Res>
  update: (id: string, data: UpdateOf<T>) => Promise<Res>
  remove: (id: string) => Promise<void>
  batch: (arr: Partial<InsertOf<T>>[]) => Promise<Res[]>
  uploadFile: (bucket: string, path: string, file: File) => Promise<string>
}

// --- Hook implementation ---------------------------------------------
export function useSupabaseCrud<
  T extends TableName,
  Req extends Partial<InsertOf<T>> = Partial<InsertOf<T>>,
  Res extends RowOf<T> = RowOf<T>
>({
  table,
  requestSchema,
  responseSchema,
  responseListSchema,
  filters,
  order,
  pagination,
  maxRetries = 3,
  retryDelay = 500,
  upstash = { addHeaders: true },
  onSuccess,
  onError,
}: UseSupabaseCrudOptions<T, Req, Res>): UseSupabaseCrudReturn<T, Res> {
  const toast = useToast().toast
  const { useUpstashAdapter } = useMemoryProvider()

  // Determine if we should use Upstash adapter
  const shouldUseUpstash = upstash.forceUse !== undefined
    ? upstash.forceUse
    : useUpstashAdapter

  // Create a ref to hold the client
  const clientRef = useRef<SupabaseClient<Database> | UpstashSupabaseClient | null>(null)

  // Initialize the client if not already done
  if (!clientRef.current) {
    if (shouldUseUpstash) {
      try {
        clientRef.current = createSupabaseClient()
        console.log("Using Upstash adapter for Supabase CRUD operations")
      } catch (error) {
        console.error("Error creating Upstash adapter:", error)
        // Fall back to regular Supabase client
        clientRef.current = getSupabaseClient() as SupabaseClient<Database>
      }
    } else {
      clientRef.current = getSupabaseClient() as SupabaseClient<Database>
    }
  }

  // Type guard to check if client is Upstash adapter
  const isUpstashAdapter = (client: any): client is UpstashSupabaseClient => {
    return client && typeof client === 'object' && 'isUpstashAdapter' in client && client.isUpstashAdapter === true;
  }

  const [items, setItems] = useState<Res[]>([])
  const [loading, setLoading] = useState<boolean>(false)
  const [error, setError] = useState<Error | null>(null)
  const retryCount = useRef<number>(0)

  // Zod validation helper
  const validate = useCallback(
    <S>(schema: z.ZodSchema<S> | undefined, data: unknown): S => {
      if (!schema) return data as S
      return schema.parse(data)
    },
    []
  )

  // retry/backoff wrapper
  const withRetry = useCallback(
    async <R>(op: CrudOp, fn: () => Promise<R> | R): Promise<R> => {
      try {
        const result = await fn()
        retryCount.current = 0
        return result
      } catch (err: any) {
        const isNetwork =
          err instanceof Error && /(fetch|network|\d{3})/i.test(err.message)
        if (isNetwork && retryCount.current < maxRetries) {
          retryCount.current++
          await new Promise((r) =>
            setTimeout(r, retryDelay * 2 ** (retryCount.current - 1))
          )
          return withRetry(op, fn)
        }
        retryCount.current = 0
        throw err
      }
    },
    [maxRetries, retryDelay]
  )

  // --- FETCH ALL ------------------------------------------------------
  const fetchAll = useCallback(async (): Promise<Res[]> => {
    setLoading(true)
    setError(null)
    try {
      // Check if client is available
      if (!clientRef.current) {
        throw new Error("Database client is not available");
      }

      const { data, error: pgErr } = await withRetry("fetch" as const, async () => {
        // Use type assertion to handle both Supabase and Upstash adapter clients
        const client = clientRef.current as any;
        let q = client.from(table).select("*")
        if (filters) {
          for (const [col, val] of Object.entries(filters)) {
            q = q.eq(col as any, val as any)
          }
        }
        if (order) {
          q = q.order(order.column as string, {
            ascending: order.ascending ?? true,
          })
        }
        if (pagination) {
          q = q.range(pagination.offset, pagination.offset + pagination.limit - 1)
        }
        return q
      })
      if (pgErr) throw pgErr
      const rows = responseListSchema
        ? validate(responseListSchema, data)
        : (data as unknown as Res[])

      setItems(rows)
      onSuccess?.("fetch", rows)
      return rows
    } catch (err: any) {
      setError(err)
      onError?.(err, "fetch")
      toast({
        title: "Fetch Error",
        description: err.message,
        variant: "destructive",
      })
      throw err
    } finally {
      setLoading(false)
    }
  }, [
    clientRef,
    table,
    filters,
    order,
    pagination,
    responseListSchema,
    validate,
    onSuccess,
    onError,
    toast,
    withRetry,
  ])

  // --- CREATE ---------------------------------------------------------
  const create = useCallback(
    async (data: Partial<InsertOf<T>>): Promise<Res> => {
      setLoading(true)
      setError(null)

      // Check if client is available
      if (!clientRef.current) {
        throw new Error("Database client is not available");
      }

      let validated: Partial<InsertOf<T>>
      try {
        validated = validate(requestSchema as z.ZodSchema<Partial<InsertOf<T>>>, data)
      } catch (zErr: any) {
        setError(zErr)
        onError?.(zErr, "create")
        toast({
          title: "Validation Error",
          description: zErr.message,
          variant: "destructive",
        })
        throw zErr
      }

      return withRetry("create" as const, async () => {
        try {
          // Use type assertion to handle both Supabase and Upstash adapter clients
          const client = clientRef.current as any;
          const { data: result, error: pgErr } = await client
            .from(table)
            .insert([validated as any])
            .select("*")
            .single()

          if (pgErr) throw pgErr
          if (!result) throw new Error("No data returned")

          const row = validate(responseSchema, result) as Res
          setItems((cur) => [row, ...cur])
          onSuccess?.("create", row)
          toast({ title: "Created", description: "Record created." })
          return row
        } catch (err: any) {
          setError(err)
          onError?.(err, "create")
          toast({
            title: "Create Error",
            description: err.message,
            variant: "destructive",
          })
          throw err
        } finally {
          setLoading(false)
        }
      })
    },
    [
      clientRef,
      table,
      requestSchema,
      responseSchema,
      validate,
      onSuccess,
      onError,
      toast,
      withRetry,
    ]
  )

  // --- UPDATE ---------------------------------------------------------
  const update = useCallback(
    async (id: string, changes: UpdateOf<T>): Promise<Res> => {
      setLoading(true)
      setError(null)

      // Check if client is available
      if (!clientRef.current) {
        throw new Error("Database client is not available");
      }

      let validated: UpdateOf<T>
      try {
        validated = validate(responseSchema as z.ZodSchema<UpdateOf<T>>, { ...changes, id } as any)
      } catch (zErr: any) {
        setError(zErr)
        onError?.(zErr, "update")
        toast({
          title: "Validation Error",
          description: zErr.message,
          variant: "destructive",
        })
        throw zErr
      }

      try {
        const result = await withRetry("update" as const, async () => {
          const updateData = { ...validated };
          if ('id' in updateData) {
            delete updateData.id;
          }
          // Use type assertion to handle both Supabase and Upstash adapter clients
          const client = clientRef.current as any;
          return await client
            .from(table)
            .update(updateData as any)
            .eq("id", id as any)
            .select("*")
            .single()
        })

        if (result.error) throw result.error
        if (!result.data) throw new Error("No data returned")

        const row = validate(responseSchema, result.data)
        setItems((cur) => cur.map((r) => ('id' in r && r.id === id ? row : r)))
        onSuccess?.("update", row)
        toast({ title: "Updated", description: "Record updated." })
        return row
      } catch (err: any) {
        setError(err)
        onError?.(err, "update")
        toast({
          title: "Update Error",
          description: err.message,
          variant: "destructive",
        })
        throw err
      } finally {
        setLoading(false)
      }
    },
    [
      clientRef,
      table,
      responseSchema,
      validate,
      onSuccess,
      onError,
      toast,
      withRetry,
    ]
  )

  // --- DELETE ---------------------------------------------------------
  const remove = useCallback(
    async (id: string): Promise<void> => {
      setLoading(true)
      setError(null)

      // Check if client is available
      if (!clientRef.current) {
        throw new Error("Database client is not available");
      }

      try {
        const result = await withRetry("delete" as const, async () => {
          // Use type assertion to handle both Supabase and Upstash adapter clients
          const client = clientRef.current as any;
          return await client
            .from(table)
            .delete()
            .eq("id", id as any)
            .select("*")
            .single();
        });
        if (result.error) throw result.error;

        setItems((cur) => cur.filter((r) => 'id' in r && r.id !== id));
        onSuccess?.("delete");
        toast({ title: "Deleted", description: "Record deleted." });
      } catch (err: any) {
        setError(err);
        onError?.(err, "delete");
        toast({
          title: "Delete Error",
          description: err.message,
          variant: "destructive",
        });
        throw err;
      } finally {
        setLoading(false);
      }
    },
    [clientRef, table, onSuccess, onError, toast, withRetry]
  )

  // --- BATCH INSERT ---------------------------------------------------
  const batch = useCallback(
    async (arr: Partial<InsertOf<T>>[]): Promise<Res[]> => {
      setLoading(true)
      setError(null)

      // Check if client is available
      if (!clientRef.current) {
        throw new Error("Database client is not available");
      }

      let validatedArr: Partial<InsertOf<T>>[]
      try {
        validatedArr = arr.map((d) => validate(requestSchema as z.ZodSchema<Partial<InsertOf<T>>>, d))
      } catch (zErr: any) {
        setError(zErr)
        onError?.(zErr, "batch")
        toast({
          title: "Validation Error",
          description: zErr.message,
          variant: "destructive",
        })
        throw zErr
      }

      try {
        const result = await withRetry("batch" as const, async () => {
          // Use type assertion to handle both Supabase and Upstash adapter clients
          const client = clientRef.current as any;
          return await client
            .from(table)
            .insert(validatedArr as any)
            .select("*")
        })

        if (result.error) throw result.error
        if (!result.data) throw new Error("No data returned")

        const rows = responseListSchema
          ? validate(responseListSchema, result.data)
          : (result.data as unknown as Res[])

        setItems((cur) => [...rows, ...cur])
        onSuccess?.("batch", rows)
        toast({
          title: "Batch Created",
          description: `${rows.length} records created.`,
        })
        return rows
      } catch (err: any) {
        setError(err)
        onError?.(err, "batch")
        toast({
          title: "Batch Error",
          description: err.message,
          variant: "destructive",
        })
        throw err
      } finally {
        setLoading(false)
      }
    },    [
      clientRef,
      table,
      requestSchema,
      responseListSchema,
      validate,
      onSuccess,
      onError,
      toast,
      withRetry,
    ]
  )

  // --- FILE UPLOAD ----------------------------------------------------
  const uploadFile = useCallback(
    async (bucket: string, path: string, file: File): Promise<string> => {
      setLoading(true)
      setError(null)

      // Check if client is available
      if (!clientRef.current) {
        throw new Error("Database client is not available");
      }

      try {
        // Get a regular Supabase client for storage operations
        // This is because the Upstash adapter might not support storage operations
        const storageClient = getSupabaseClient() as any

        const { error: stErr } = await storageClient.storage
          .from(bucket)
          .upload(path, file)
        if (stErr) throw stErr as unknown as PostgrestError

        const url = storageClient.storage.from(bucket).getPublicUrl(path).data.publicUrl
        onSuccess?.("upload", url)
        toast({ title: "Uploaded", description: "File uploaded." })
        return url
      } catch (err: any) {
        const e = err instanceof Error ? err : new Error(String(err))
        setError(e)
        onError?.(e, "upload")
        toast({
          title: "Upload Error",
          description: e.message,
          variant: "destructive",
        })
        throw e
      } finally {
        setLoading(false)
      }
    },
    [clientRef, onSuccess, onError, toast]
  )

  // auto‐fetch on mount
  useEffect(() => {
    fetchAll().catch(() => {})
  }, [fetchAll])

  return {
    items,
    loading,
    error,
    fetchAll,
    create,
    update,
    remove,
    batch,
    uploadFile,
  }
}

================
File: hooks/use-supabase-fetch.ts
================
"use client"

/**
 * Enhanced hook for fetching data from a Next.js API route that, in turn,
 * talks to Supabase or Upstash (via the Upstash adapter).  Supports:
 * – cursor or page-based pagination
 * – retries with exponential back-off
 * – optional in-memory LRU caching
 * – automatic detection of Upstash adapter
 *
 * @module hooks/use-supabase-fetch
 */

import { useState, useEffect, useRef, useCallback } from "react"
import { useToast } from "@/hooks/use-toast"
import { LRUCache } from "lru-cache"
import { useMemoryProvider } from "./use-memory-provider"

/* -------------------------------------------------------------------------- */
/* Types                                                                      */
/* -------------------------------------------------------------------------- */

interface UseSupabaseFetchOptions<T> {
  endpoint: string                              // API route   (e.g. /api/content/hero)
  resourceName: string                          // Human readable – used in toasts
  dataKey: string                               // Key containing the array in the response JSON

  initialData?: T[]
  queryParams?: Record<string, string>

  enabled?: boolean

  /* retry behaviour */
  maxRetries?: number
  retryDelay?: number                           // base delay in ms (exponential back-off)

  /* realtime subscriptions – currently unused but reserved for future use */
  realtime?: boolean

  pagination?: {
    pageSize?: number
    useCursor?: boolean
    initialCursor?: string
  }

  sort?: {
    column: string
    ascending?: boolean
  }[]

  cache?: {
    enabled?: boolean
    ttl?: number
    maxSize?: number
  }

  /* Upstash adapter options */
  upstash?: {
    /**
     * Whether to force using Upstash adapter
     * If not specified, will use the value from useMemoryProvider
     */
    forceUse?: boolean

    /**
     * Whether to add Upstash adapter headers to the request
     * @default true
     */
    addHeaders?: boolean
  }

  onSuccess?: (data: T[]) => void
  onError?: (error: Error) => void
}

/* -------------------------------------------------------------------------- */
/* Hook                                                                        */
/* -------------------------------------------------------------------------- */

export function useSupabaseFetch<T>({
  endpoint,
  resourceName,
  dataKey,
  initialData = [],
  queryParams = {},

  /* control */
  enabled = true,

  /* retry */
  maxRetries = 3,
  retryDelay = 1_000,

  /* pagination */
  pagination = { pageSize: 20, useCursor: false },
  sort,

  /* cache */
  cache = { enabled: true, ttl: 60_000, maxSize: 100 },

  /* upstash */
  upstash = { addHeaders: true },

  onSuccess,
  onError,
}: UseSupabaseFetchOptions<T>) {
  const { toast } = useToast()
  const { useUpstashAdapter } = useMemoryProvider()

  /* ---------------------------------------------------------------------- */
  /* State                                                                  */
  /* ---------------------------------------------------------------------- */

  const [data, setData]               = useState<T[]>(initialData)
  const [isLoading, setIsLoading]     = useState(true)
  const [error, setError]             = useState<Error | null>(null)
  const [connectionError, setConnErr] = useState(false)

  /* pagination helpers */
  const [totalCount, setTotalCount]   = useState<number | null>(null)
  const [hasMore, setHasMore]         = useState(true)
  const [cursor, setCursor]           = useState<string | null>(
    pagination.useCursor ? pagination.initialCursor ?? null : null
  )
  const [page, setPage]               = useState(1)

  /* LRU cache */
  const cacheRef = useRef(
    new LRUCache<string, {
      data: T[]
      totalCount: number | null
      hasMore: boolean
      nextCursor: string | null
    }>({
      max:   cache.maxSize ?? 100,
      ttl:   cache.ttl     ?? 60_000,
      updateAgeOnGet: true,
    })
  )

  /* ---------------------------------------------------------------------- */
  /* Main fetch function (memoised)                                         */
  /* ---------------------------------------------------------------------- */

  const fetchData = useCallback(
    async (retryCount = 0, loadMore = false): Promise<void> => {
      if (!enabled) {
        setIsLoading(false)
        return
      }

      /* for infinite scroll we don’t want a global spinner */
      if (!loadMore) setIsLoading(true)

      setError(null)
      setConnErr(false)

      /* build cache key */
      const cacheKey = `${endpoint}_${JSON.stringify(queryParams)}_${page}_${cursor}`

      try {
        /* ---------------------------- 1. cache hit ----------------------- */
        if (cache.enabled) {
          const cached = cacheRef.current.get(cacheKey)
          if (cached) {
            if (loadMore) {
              setData(prev => [...prev, ...cached.data])
            } else {
              setData(cached.data)
            }

            setTotalCount(cached.totalCount)
            setHasMore(cached.hasMore)
            setCursor(cached.nextCursor)
            setIsLoading(false)
            return
          }
        }

        /* ------------------------- 2. build URL -------------------------- */
        const url = new URL(endpoint, window.location.origin)

        /* base params */
        Object.entries(queryParams).forEach(([k, v]) =>
          url.searchParams.append(k, v)
        )

        /* pagination */
        if (pagination.pageSize)
          url.searchParams.append("pageSize", String(pagination.pageSize))

        if (pagination.useCursor && cursor) {
          url.searchParams.append("cursor", cursor)
        } else if (!pagination.useCursor) {
          url.searchParams.append("page", String(page))
        }

        /* sorting */
        if (sort?.length) {
          const s = sort.map(({ column, ascending = true }) =>
            `${column}:${ascending ? "asc" : "desc"}`
          ).join(",")
          url.searchParams.append("sort", s)
        }

        /* ---------------------- 3. perform request ----------------------- */
        // Add Upstash adapter headers if enabled
        const headers: HeadersInit = {};

        // Check if we should use Upstash adapter
        const shouldUseUpstash = upstash.forceUse !== undefined
          ? upstash.forceUse
          : useUpstashAdapter;

        if (shouldUseUpstash) {
          if (upstash.addHeaders) {
            headers['x-use-upstash-adapter'] = 'true';

            // Add additional headers for Upstash adapter configuration
            headers['x-upstash-adapter-version'] = '1.0.0';

            // Add cache control headers
            headers['x-upstash-cache-control'] = cache.enabled ? 'enabled' : 'disabled';

            if (cache.enabled && cache.ttl) {
              headers['x-upstash-cache-ttl'] = String(cache.ttl);
            }
          }

          // Log Upstash adapter usage for debugging
          console.log(`Using Upstash adapter for fetch: ${endpoint}`);
        }

        const response = await fetch(url, { headers })

        if (!response.ok) {
          const { error: msg } = await response.json().catch(() => ({}))
          throw new Error(msg ?? `Failed to fetch ${resourceName.toLowerCase()}`)
        }

        const result = await response.json()

        /* ---------------------- 4. parse / validate ---------------------- */
        if (!(dataKey in result)) {
          throw new Error(
            `Invalid response format: missing "${dataKey}" in ${resourceName}`
          )
        }

        const newData: T[]          = result[dataKey]
        const total: number | null  = result.totalCount ?? null
        const nextCursor            = result.nextCursor ?? null
        const more                  = result.hasMore ?? false

        /* update state */
        setData(prev => (loadMore ? [...prev, ...newData] : newData))
        setTotalCount(total)
        setHasMore(more)
        if (pagination.useCursor) setCursor(nextCursor)

        /* cache */
        if (cache.enabled) {
          cacheRef.current.set(cacheKey, {
            data: newData,
            totalCount: total,
            hasMore: more,
            nextCursor,
          })
        }

        onSuccess?.(newData)
      } catch (err) {
        const e = err instanceof Error
          ? err
          : new Error(`Failed to fetch ${resourceName.toLowerCase()}`)

        const isNetworkErr =
          e.message.includes("fetch")    ||
          e.message.includes("network")  ||
          e.message.includes("HTTP error 5")

        /* retry with exponential back-off */
        if (isNetworkErr && retryCount < maxRetries) {
          const delay = retryDelay * 2 ** retryCount
          setTimeout(() => fetchData(retryCount + 1, loadMore), delay)
          return
        }

        setError(e)
        setConnErr(true)
        toast({
          title:   `Failed to fetch ${resourceName.toLowerCase()}`,
          description: isNetworkErr
            ? "Could not connect to the backend. Check your connection."
            : e.message,
          variant: "destructive",
        })

        onError?.(e)
      } finally {
        setIsLoading(false)
      }
    },
    [
      /* deps */
      enabled,
      endpoint,
      resourceName,
      dataKey,
      JSON.stringify(queryParams),   // safe because queryParams is shallow
      maxRetries,
      retryDelay,
      page,
      cursor,
      pagination.pageSize,
      pagination.useCursor,
      JSON.stringify(sort),
      cache.enabled,
      toast,
      onSuccess,
      onError,
      useUpstashAdapter,
      upstash.forceUse,
      upstash.addHeaders,
    ]
  )

  /* -------------------------------------------------------------- */
  /* Effects                                                        */
  /* -------------------------------------------------------------- */

  /* initial + reactive fetch */
  useEffect(() => {
    fetchData()
  }, [fetchData])

  /* -------------------------------------------------------------- */
  /* Public API                                                     */
  /* -------------------------------------------------------------- */

  const fetchMore = () => {
    if (pagination.useCursor) {
      fetchData(0, true)
    } else {
      setPage(p => p + 1)
      fetchData(0, true)
    }
  }

  const refetch = () => fetchData()

  return {
    data,
    isLoading,
    error,
    connectionError,

    totalCount,
    hasMore,

    fetchMore,
    refetch,
  }
}




================================================================
End of Codebase
================================================================
